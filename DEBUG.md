# ç›‘æ§æ ˆéƒ¨ç½²é—®é¢˜æ’æŸ¥æŒ‡å—

æœ¬æ–‡æ¡£è®°å½•äº†åœ¨éƒ¨ç½²ç›‘æ§æ ˆè¿‡ç¨‹ä¸­é‡åˆ°çš„é—®é¢˜ã€åŸå› åˆ†æå’Œè§£å†³æ–¹æ¡ˆã€‚

## ğŸ“‹ é—®é¢˜æ¦‚è§ˆ

åœ¨åˆå§‹éƒ¨ç½²åï¼ŒArgoCD åº”ç”¨çŠ¶æ€æ˜¾ç¤ºä»¥ä¸‹é—®é¢˜ï¼š

| åº”ç”¨åç§°       | åŒæ­¥çŠ¶æ€ | å¥åº·çŠ¶æ€ | é—®é¢˜æè¿°                          |
| -------------- | -------- | -------- | --------------------------------- |
| loki           | Unknown  | Healthy  | æ— æ³•ç”Ÿæˆæ¸…å•ï¼šéœ€è¦å¯¹è±¡å­˜å‚¨åç«¯    |
| nginx-test-app | Unknown  | Healthy  | æ‰¾ä¸åˆ° values æ–‡ä»¶è·¯å¾„            |
| prometheus     | Synced   | Degraded | Grafana Pod æ— æ³•å¯åŠ¨ï¼šç¼ºå°‘ Secret |
| promtail       | Synced   | Healthy  | âœ… æ­£å¸¸                           |

---

## ğŸ” é—®é¢˜ 1: Loki - å¯¹è±¡å­˜å‚¨åç«¯é”™è¯¯å’Œç»„ä»¶ Pending é—®é¢˜

### é”™è¯¯ä¿¡æ¯

**åˆå§‹é”™è¯¯ï¼š**

```
Failed to load target state: failed to generate manifest for source 1 of 2:
rpc error: code = Unknown desc = Manifest generation error (cached):
failed to execute helm template command:
Error: execution error at (loki/templates/validate.yaml:19:4):
Cannot run scalable targets (backend, read, write) or distributed targets
without an object storage backend.
```

**åç»­é—®é¢˜ï¼š**

- `loki-chunks-cache-0` Pod å¤„äº `Pending` çŠ¶æ€
- Application å¥åº·çŠ¶æ€æ˜¾ç¤ºä¸º `Progressing` è€Œä¸æ˜¯ `Healthy`
- Helm æ¨¡æ¿éªŒè¯é”™è¯¯ï¼š"You have more than zero replicas configured for both the single binary and simple scalable targets"

### åŸå› åˆ†æ

1. **åˆå§‹é—®é¢˜**ï¼šLoki Helm Chart 6.0.0 ç‰ˆæœ¬é»˜è®¤ä½¿ç”¨åˆ†å¸ƒå¼æ¨¡å¼ï¼ˆdistributed modeï¼‰ï¼Œè¯¥æ¨¡å¼éœ€è¦é…ç½®å¯¹è±¡å­˜å‚¨åç«¯ï¼ˆå¦‚ S3ã€GCSã€Azure Blob ç­‰ï¼‰ã€‚ä½†æˆ‘ä»¬çš„é…ç½®ä½¿ç”¨çš„æ˜¯ `filesystem` å­˜å‚¨ç±»å‹ï¼Œè¿™ä¼šå¯¼è‡´éªŒè¯å¤±è´¥ã€‚

2. **Pending é—®é¢˜**ï¼šåœ¨ SingleBinary æ¨¡å¼ä¸‹ï¼Œç¼“å­˜ç»„ä»¶ï¼ˆchunksCacheã€resultsCacheï¼‰å’Œ Gateway ä¸æ˜¯å¿…éœ€çš„ï¼Œä½† Helm Chart é»˜è®¤ä¼šå°è¯•åˆ›å»ºå®ƒä»¬ï¼Œå¯¼è‡´èµ„æºåˆ†é…é—®é¢˜æˆ–é…ç½®å†²çªã€‚

3. **Replicas å†²çªé—®é¢˜**ï¼šå³ä½¿è®¾ç½®äº† `simpleScalable.enabled: false`ï¼Œå¦‚æœ `replicas` æ²¡æœ‰æ˜¾å¼è®¾ç½®ä¸º `0`ï¼ŒHelm Chart éªŒè¯ä¼šå¤±è´¥ï¼Œå› ä¸ºé»˜è®¤å€¼å¯èƒ½ä¸æ˜¯ 0ã€‚

### è§£å†³æ–¹æ¡ˆ

åœ¨ `monitoring/values/loki-values.yaml` ä¸­å¯ç”¨å•å®ä¾‹æ¨¡å¼ï¼ˆsingleBinaryï¼‰ï¼Œå¹¶**å¿…é¡»**ç¦ç”¨æ‰€æœ‰ä¸å¿…è¦çš„ç»„ä»¶ï¼š

```yaml
# Loki é…ç½®
loki:
  auth_enabled: false
  commonConfig:
    replication_factor: 1
  storage:
    type: filesystem
  limits_config:
    retention_period: 720h
    ingestion_rate_mb: 16
    ingestion_burst_size_mb: 32
    max_query_parallelism: 32
    max_query_series: 500

# ä½¿ç”¨å•å®ä¾‹æ¨¡å¼ï¼Œä¸éœ€è¦å¯¹è±¡å­˜å‚¨
# é‡è¦ï¼šå¿…é¡»è®¾ç½® deploymentModeï¼Œå¦åˆ™ä¼šæŠ¥é”™
deploymentMode: SingleBinary

singleBinary:
  enabled: true
  replicas: 1

# ç¦ç”¨å…¶ä»–éƒ¨ç½²æ¨¡å¼ï¼ˆå¿…é¡»æ˜¾å¼ç¦ç”¨ï¼Œä¸” replicas å¿…é¡»è®¾ç½®ä¸º 0ï¼‰
simpleScalable:
  enabled: false
  replicas: 0 # å¿…é¡»æ˜¾å¼è®¾ç½®ä¸º 0ï¼Œå¦åˆ™ä¼šä¸ singleBinary å†²çª
read:
  enabled: false
  replicas: 0
write:
  enabled: false
  replicas: 0
backend:
  enabled: false
  replicas: 0

# æŒä¹…åŒ–å­˜å‚¨
persistence:
  enabled: true
  storageClassName: do-block-storage
  size: 50Gi

# èµ„æºé™åˆ¶
resources:
  requests:
    cpu: 200m
    memory: 512Mi
  limits:
    cpu: 1000m
    memory: 2Gi

# Service é…ç½®
service:
  type: ClusterIP
  port: 3100

# ç¦ç”¨æ‰€æœ‰ç¼“å­˜ç»„ä»¶ï¼ˆSingleBinary æ¨¡å¼ä¸éœ€è¦ï¼‰
chunksCache:
  enabled: false

resultsCache:
  enabled: false

# ç¦ç”¨ Gatewayï¼ˆSingleBinary æ¨¡å¼ç›´æ¥ä½¿ç”¨ Serviceï¼‰
# ç»„ä»¶åº”è¯¥ç›´æ¥è®¿é—® loki Service: http://loki.monitoring.svc:3100
gateway:
  enabled: false

# ç¦ç”¨ Canaryï¼ˆæµ‹è¯•ç»„ä»¶ï¼Œéå¿…éœ€ï¼‰
canary:
  enabled: false

# ç¦ç”¨å…¶ä»–ä¸å¿…è¦çš„ç»„ä»¶
monitoring:
  dashboards:
    enabled: false
  rules:
    enabled: false
  serviceMonitor:
    enabled: false
```

**å…³é”®ç‚¹ï¼š**

- `deploymentMode: SingleBinary` æ˜¯å¿…éœ€çš„ï¼Œå‘Šè¯‰ Helm Chart ä½¿ç”¨å•å®ä¾‹æ¨¡å¼
- å¿…é¡»æ˜¾å¼ç¦ç”¨å…¶ä»–æ¨¡å¼ï¼ˆsimpleScalable, read, write, backendï¼‰ï¼Œ**ä¸”å¿…é¡»å°† replicas è®¾ç½®ä¸º 0**ï¼Œå¦åˆ™ Helm Chart éªŒè¯ä¼šå¤±è´¥
- **å¿…é¡»ç¦ç”¨ç¼“å­˜ç»„ä»¶**ï¼ˆchunksCacheã€resultsCacheï¼‰ï¼Œå¦åˆ™ä¼šå¯¼è‡´ Pod Pending
- **å»ºè®®ç¦ç”¨ Gateway**ï¼Œè®©ç»„ä»¶ç›´æ¥è®¿é—® Loki Serviceï¼Œç®€åŒ–æ¶æ„
- å¦‚æœåªè®¾ç½® `singleBinary.enabled: true` è€Œä¸è®¾ç½® `deploymentMode`ï¼Œä¼šå‡ºç°é”™è¯¯ï¼š"You have more than zero replicas configured for both the single binary and simple scalable targets"
- **é‡è¦**ï¼šå³ä½¿ `enabled: false`ï¼Œä¹Ÿå¿…é¡»æ˜¾å¼è®¾ç½® `replicas: 0`ï¼Œå› ä¸º Helm Chart çš„é»˜è®¤å€¼å¯èƒ½ä¸æ˜¯ 0

### éªŒè¯

```bash
# æ£€æŸ¥ Loki Application çŠ¶æ€
kubectl get application loki -n argocd

# æ£€æŸ¥ Loki Pod çŠ¶æ€
kubectl get pods -n monitoring -l app.kubernetes.io/name=loki

# æŸ¥çœ‹ Loki æ—¥å¿—
kubectl logs -n monitoring -l app.kubernetes.io/name=loki --tail=50
```

---

## ğŸ” é—®é¢˜ 2: nginx-test-app - æ‰¾ä¸åˆ° values æ–‡ä»¶

### é”™è¯¯ä¿¡æ¯

```
Failed to load target state: failed to generate manifest for source 1 of 1:
rpc error: code = Unknown desc = Manifest generation error (cached):
failed to execute helm template command:
Error: open /tmp/.../nginx/test-app/values/nginx-values.yaml:
no such file or directory
```

### åŸå› åˆ†æ

`nginx-app.yaml` ä¸­ä½¿ç”¨äº† `$values/test-app/values/nginx-values.yaml` æ¥å¼•ç”¨ values æ–‡ä»¶ï¼Œä½†é…ç½®ä¸­åªæŒ‡å®šäº† Helm Chart ä»“åº“ï¼Œæ²¡æœ‰æŒ‡å®š Git ä»“åº“ä½œä¸º values çš„æ¥æºã€‚ArgoCD æ— æ³•æ‰¾åˆ° values æ–‡ä»¶çš„ä½ç½®ã€‚

### è§£å†³æ–¹æ¡ˆ

ä¿®æ”¹ `test-app/argocd/nginx-app.yaml`ï¼Œä½¿ç”¨ `sources`ï¼ˆå¤æ•°ï¼‰è€Œä¸æ˜¯ `source`ï¼Œå¹¶æ·»åŠ  Git ä»“åº“ä½œä¸ºç¬¬äºŒä¸ª sourceï¼š

```yaml
spec:
  project: default
  sources: # æ³¨æ„ï¼šä½¿ç”¨ sourcesï¼ˆå¤æ•°ï¼‰
    - repoURL: https://charts.bitnami.com/bitnami
      chart: nginx
      targetRevision: 15.0.0
      helm:
        valueFiles:
          - $values/test-app/values/nginx-values.yaml
    - repoURL: https://github.com/leonardsun899/monitoring-stack.git
      targetRevision: main
      ref: values # è¿™ä¸ª ref å‘Šè¯‰ ArgoCD è¿™æ˜¯ values æ–‡ä»¶çš„æ¥æº
```

**å…³é”®ç‚¹ï¼š**

- ä½¿ç”¨ `sources`ï¼ˆå¤æ•°ï¼‰æ”¯æŒå¤šä¸ªä»“åº“æº
- ç¬¬ä¸€ä¸ª source æ˜¯ Helm Chart ä»“åº“
- ç¬¬äºŒä¸ª source æ˜¯ Git ä»“åº“ï¼Œç”¨äºæä¾› values æ–‡ä»¶
- `ref: values` æ ‡è¯†è¿™ä¸ª source ç”¨äº values æ–‡ä»¶

### éªŒè¯

```bash
# æ£€æŸ¥ nginx-test-app Application çŠ¶æ€
kubectl get application nginx-test-app -n argocd

# æ£€æŸ¥ Nginx Pod çŠ¶æ€
kubectl get pods -n test-app

# æ£€æŸ¥ ServiceMonitor æ˜¯å¦åˆ›å»º
kubectl get servicemonitor -n monitoring
```

---

## ğŸ” é—®é¢˜ 3: Prometheus/Grafana - Secret ä¸å­˜åœ¨

### é”™è¯¯ä¿¡æ¯

```bash
kubectl describe pod prometheus-grafana-xxx -n monitoring

Events:
  Warning  Failed  Error: secret "grafana-admin-credentials" not found
```

### åŸå› åˆ†æ

`prometheus-values.yaml` ä¸­é…ç½®äº†ï¼š

```yaml
grafana:
  admin:
    existingSecret: grafana-admin-credentials
    userKey: admin-user
    passwordKey: admin-password
```

è¿™å‘Šè¯‰ Grafana ä½¿ç”¨å·²å­˜åœ¨çš„ Secretï¼Œä½†è¯¥ Secret å¹¶ä¸å­˜åœ¨ã€‚Grafana Helm Chart åº”è¯¥è‡ªåŠ¨åˆ›å»º Secretï¼Œä½†é…ç½®ä¸­æŒ‡å®šäº† `existingSecret`ï¼Œå¯¼è‡´å®ƒä¸ä¼šè‡ªåŠ¨åˆ›å»ºã€‚

### è§£å†³æ–¹æ¡ˆ

**å®Œå…¨ç§»é™¤ `admin` é…ç½®éƒ¨åˆ†**ï¼Œåªä¿ç•™ `secret` é…ç½®ã€‚å¦‚æœä¿ç•™ç©ºçš„ `admin:` é…ç½®ï¼Œä¼šå¯¼è‡´ Helm æ¨¡æ¿é”™è¯¯ï¼š

```yaml
grafana:
  enabled: true
  # ä¸é…ç½® admin éƒ¨åˆ†ï¼Œè®© Helm chart ä½¿ç”¨é»˜è®¤é…ç½®
  # admin é…ç½®ä¼šå¯¼è‡´æ¨¡æ¿é”™è¯¯ï¼Œä½¿ç”¨ secret é…ç½®å³å¯
  secret:
    admin-user: admin
    admin-password: "admin" # ç”Ÿäº§ç¯å¢ƒè¯·ä½¿ç”¨å¼ºå¯†ç 
```

**é”™è¯¯ç¤ºä¾‹ï¼ˆä¼šå¯¼è‡´æ¨¡æ¿é”™è¯¯ï¼‰ï¼š**

```yaml
grafana:
  enabled: true
  admin:
    # å³ä½¿æ³¨é‡Šæ‰ï¼Œç©ºçš„ admin é…ç½®ä¹Ÿä¼šå¯¼è‡´é”™è¯¯
    # existingSecret: grafana-admin-credentials
  secret:
    admin-user: admin
    admin-password: "admin"
```

**é”™è¯¯ä¿¡æ¯ï¼š**

```
Error: template: kube-prometheus-stack/charts/grafana/templates/secret.yaml:1:27:
executing "kube-prometheus-stack/charts/grafana/templates/secret.yaml" at <.Values.admin.existingSecret>:
nil pointer evaluating interface {}.existingSecret
```

**è¯´æ˜ï¼š**

- å¦‚æœé…ç½®äº† `admin:` éƒ¨åˆ†ï¼ˆå³ä½¿æ˜¯ç©ºçš„ï¼‰ï¼ŒHelm Chart ä¼šå°è¯•è®¿é—® `admin.existingSecret`ï¼Œå¯¼è‡´ nil pointer é”™è¯¯
- å®Œå…¨ç§»é™¤ `admin` é…ç½®ï¼Œåªä½¿ç”¨ `secret` é…ç½®ï¼ŒHelm Chart ä¼šè‡ªåŠ¨åˆ›å»º Secret
- ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ Kubernetes Secret ç®¡ç†å·¥å…·ï¼ˆå¦‚ Sealed Secretsã€External Secretsï¼‰

### éªŒè¯

```bash
# æ£€æŸ¥ Grafana Pod çŠ¶æ€
kubectl get pods -n monitoring -l app.kubernetes.io/name=grafana

# æ£€æŸ¥ Secret æ˜¯å¦åˆ›å»º
kubectl get secret -n monitoring | grep grafana

# æŸ¥çœ‹ Grafana Pod æ—¥å¿—
kubectl logs -n monitoring -l app.kubernetes.io/name=grafana --tail=50
```

### ä¸´æ—¶è§£å†³æ–¹æ¡ˆï¼šæ‰‹åŠ¨åˆ›å»º Secret

å¦‚æœç§»é™¤äº† `admin` é…ç½®åï¼ŒHelm Chart ä»ç„¶æ²¡æœ‰è‡ªåŠ¨åˆ›å»º Secretï¼Œå¯ä»¥æ‰‹åŠ¨åˆ›å»ºï¼š

```bash
kubectl create secret generic grafana-admin-credentials -n monitoring \
  --from-literal=admin-user=admin \
  --from-literal=admin-password=admin

# ç„¶ååˆ é™¤ Pod è®©å®ƒé‡æ–°åˆ›å»º
kubectl delete pod -n monitoring -l app.kubernetes.io/name=grafana
```

**æ³¨æ„**: è¿™ä¸ª Secret åç§° `grafana-admin-credentials` æ˜¯ Grafana Helm Chart çš„é»˜è®¤åç§°ã€‚å¦‚æœé…ç½®äº†ä¸åŒçš„åç§°ï¼Œéœ€è¦ç›¸åº”ä¿®æ”¹ã€‚

---

## ğŸ” é—®é¢˜ 4: Grafana - æ•°æ®æºé…ç½®é”™è¯¯

### é”™è¯¯ä¿¡æ¯

```bash
kubectl logs -n monitoring prometheus-grafana-xxx -c grafana

Error: âœ— Datasource provisioning error: datasource.yaml config is invalid.
Only one datasource per organization can be marked as default
```

### åŸå› åˆ†æ

åœ¨ `prometheus-values.yaml` ä¸­é…ç½®äº†å¤šä¸ªæ•°æ®æºï¼ˆPrometheus å’Œ Lokiï¼‰ï¼Œå¦‚æœéƒ½è®¾ç½®äº† `isDefault: true`ï¼ŒGrafana ä¼šæŠ¥é”™ï¼Œå› ä¸ºæ¯ä¸ªç»„ç»‡åªèƒ½æœ‰ä¸€ä¸ªé»˜è®¤æ•°æ®æºã€‚

### è§£å†³æ–¹æ¡ˆ

ç¡®ä¿åªæœ‰ä¸€ä¸ªæ•°æ®æºè®¾ç½®ä¸º `isDefault: true`ï¼Œå…¶ä»–æ•°æ®æºè®¾ç½®ä¸º `isDefault: false` æˆ–ä¸è®¾ç½®ï¼ˆé»˜è®¤ä¸º falseï¼‰ï¼š

```yaml
grafana:
  datasources:
    datasources.yaml:
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          access: proxy
          url: http://prometheus-operated.monitoring.svc:9090
          isDefault: true # åªæœ‰ Prometheus è®¾ç½®ä¸ºé»˜è®¤
          editable: true
        - name: Loki
          type: loki
          access: proxy
          url: http://loki.monitoring.svc:3100
          isDefault: false # é‡è¦ï¼šå¿…é¡»è®¾ç½®ä¸º false
          editable: true
```

**å…³é”®ç‚¹ï¼š**

- åªèƒ½æœ‰ä¸€ä¸ªæ•°æ®æºçš„ `isDefault: true`
- å…¶ä»–æ•°æ®æºå¿…é¡»æ˜¾å¼è®¾ç½® `isDefault: false` æˆ–ä¸è®¾ç½®è¯¥å­—æ®µ
- é€šå¸¸ Prometheus ä½œä¸ºé»˜è®¤æ•°æ®æºï¼Œå› ä¸ºå¤§å¤šæ•°æŸ¥è¯¢éƒ½æ˜¯ PromQL

### éªŒè¯

```bash
# æ£€æŸ¥ Grafana Pod çŠ¶æ€
kubectl get pods -n monitoring -l app.kubernetes.io/name=grafana

# æŸ¥çœ‹ Grafana æ—¥å¿—ï¼Œç¡®è®¤æ²¡æœ‰æ•°æ®æºé”™è¯¯
kubectl logs -n monitoring -l app.kubernetes.io/name=grafana -c grafana --tail=50 | grep -i datasource

# å¦‚æœ Pod åœ¨ CrashLoopBackOffï¼ŒæŸ¥çœ‹å®Œæ•´æ—¥å¿—
kubectl logs -n monitoring -l app.kubernetes.io/name=grafana -c grafana --tail=100
```

---

## ğŸ” é—®é¢˜ 5: ArgoCD Server æ— æ³•å¤–éƒ¨è®¿é—®

### é—®é¢˜æè¿°

é»˜è®¤æƒ…å†µä¸‹ï¼ŒArgoCD Server ä½¿ç”¨ ClusterIP ç±»å‹ï¼Œåªèƒ½é€šè¿‡ `kubectl port-forward` åœ¨æœ¬åœ°è®¿é—®ã€‚å¦‚æœéœ€è¦ä»å¤–éƒ¨ç½‘ç»œè®¿é—®ï¼Œéœ€è¦å°†å…¶æ”¹ä¸º LoadBalancer ç±»å‹ã€‚

### è§£å†³æ–¹æ¡ˆ

**æ–¹å¼ 1: ä½¿ç”¨é…ç½®æ–‡ä»¶ï¼ˆæ¨èï¼ŒæŒä¹…åŒ–ï¼‰**

```bash
# åº”ç”¨ Service é…ç½®
kubectl apply -f argocd/argocd-server-service.yaml

# ç­‰å¾… LoadBalancer åˆ†é… IP
kubectl get svc -n argocd argocd-server -w
```

**æ–¹å¼ 2: ä½¿ç”¨ kubectl patchï¼ˆä¸´æ—¶ï¼‰**

```bash
# ä¸´æ—¶ä¿®æ”¹ä¸º LoadBalancer
kubectl patch svc argocd-server -n argocd -p '{"spec":{"type":"LoadBalancer"}}'
```

**æ³¨æ„**:

- ä½¿ç”¨é…ç½®æ–‡ä»¶çš„æ–¹å¼æ›´å¥½ï¼Œå› ä¸ºé…ç½®ä¿å­˜åœ¨ Git ä»“åº“ä¸­ï¼Œå¯ä»¥ç‰ˆæœ¬æ§åˆ¶
- ä½¿ç”¨ patch çš„æ–¹å¼åœ¨ ArgoCD é‡æ–°åŒæ­¥æ—¶å¯èƒ½ä¼šè¢«è¦†ç›–

### è·å– LoadBalancer åœ°å€

```bash
# è·å– LoadBalancer IP æˆ– Hostname
kubectl get svc -n argocd argocd-server -o jsonpath='{.status.loadBalancer.ingress[0].ip}' && echo
# æˆ–
kubectl get svc -n argocd argocd-server -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' && echo
```

### è®¿é—® ArgoCD UI

1. ä½¿ç”¨ LoadBalancer åœ°å€è®¿é—®ï¼š

   - HTTP: `http://<loadbalancer-ip-or-hostname>`
   - HTTPS: `https://<loadbalancer-ip-or-hostname>`

2. ç™»å½•ä¿¡æ¯ï¼š
   - ç”¨æˆ·å: `admin`
   - å¯†ç : è¿è¡Œä»¥ä¸‹å‘½ä»¤è·å–
     ```bash
     kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d && echo
     ```

### å®‰å…¨å»ºè®®

âš ï¸ **ç”Ÿäº§ç¯å¢ƒå»ºè®®**:

- ä½¿ç”¨ Ingress + TLS è¯ä¹¦è€Œä¸æ˜¯ç›´æ¥æš´éœ² LoadBalancer
- é…ç½® OIDC/SSO è®¤è¯
- ä½¿ç”¨ NetworkPolicy é™åˆ¶è®¿é—®
- è€ƒè™‘ä½¿ç”¨ ClusterIP + Ingress Controllerï¼ˆå¦‚ ALBã€NGINX Ingressï¼‰

### éªŒè¯

```bash
# æ£€æŸ¥ Service ç±»å‹
kubectl get svc -n argocd argocd-server

# åº”è¯¥æ˜¾ç¤º TYPE ä¸º LoadBalancerï¼ŒEXTERNAL-IP æœ‰å€¼
# NAME            TYPE           CLUSTER-IP     EXTERNAL-IP     PORT(S)
# argocd-server   LoadBalancer   10.109.10.68   170.64.245.57   80:31797/TCP,443:32213/TCP
```

---

## ğŸ” é—®é¢˜ 6: Prometheus - EBS CSI Driver æœªå®‰è£…å¯¼è‡´ PVC æ— æ³•ç»‘å®š

### é”™è¯¯ä¿¡æ¯

```bash
kubectl get pvc -n monitoring
# STATUS: Pending
# Events: no persistent volumes available for this claim and no storage class is set

kubectl get pods -n monitoring
# prometheus-prometheus-kube-prometheus-prometheus-0: Pending
# prometheus-grafana-xxx: Pending
```

### åŸå› åˆ†æ

1. **EBS CSI Driver æœªå®‰è£…**ï¼šAWS EKS é›†ç¾¤é»˜è®¤ä¸åŒ…å« EBS CSI Driverï¼Œéœ€è¦æ‰‹åŠ¨å®‰è£…æ‰èƒ½åŠ¨æ€åˆ›å»º EBS å·
2. **PVC æ— æ³•ç»‘å®š**ï¼šæ²¡æœ‰å¯ç”¨çš„ StorageClass æˆ– CSI Driverï¼ŒPVC æ— æ³•è‡ªåŠ¨åˆ›å»º PersistentVolume
3. **Pod æ— æ³•è°ƒåº¦**ï¼šç”±äº PVC æœªç»‘å®šï¼Œä¾èµ–è¿™äº› PVC çš„ Pod æ— æ³•è°ƒåº¦

### è§£å†³æ–¹æ¡ˆ

**æ–¹å¼ 1: é€šè¿‡ Terraform å®‰è£…ï¼ˆæ¨èï¼‰**

åœ¨ `terraform/main.tf` ä¸­æ·»åŠ  EBS CSI Driver èµ„æºï¼š

```hcl
# Create IAM Role for EBS CSI Driver (IRSA)
resource "aws_iam_role" "ebs_csi_driver" {
  name = "${var.cluster_name}-ebs-csi-driver-role"

  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Effect = "Allow"
        Principal = {
          Federated = module.eks.oidc_provider_arn
        }
        Action = "sts:AssumeRoleWithWebIdentity"
        Condition = {
          StringEquals = {
            "${replace(module.eks.cluster_oidc_issuer_url, "https://", "")}:sub" = "system:serviceaccount:kube-system:ebs-csi-controller-sa"
            "${replace(module.eks.cluster_oidc_issuer_url, "https://", "")}:aud" = "sts.amazonaws.com"
          }
        }
      }
    ]
  })

  tags = {
    Environment = var.environment
    ManagedBy   = "Terraform"
  }
}

# Attach AWS managed policy for EBS CSI Driver
resource "aws_iam_role_policy_attachment" "ebs_csi_driver" {
  role       = aws_iam_role.ebs_csi_driver.name
  policy_arn = "arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy"
}

# Install EBS CSI Driver as EKS Add-on
resource "aws_eks_addon" "ebs_csi_driver" {
  cluster_name             = module.eks.cluster_name
  addon_name               = "aws-ebs-csi-driver"
  addon_version            = "v1.32.0-eksbuild.1" # Use latest compatible version
  service_account_role_arn = aws_iam_role.ebs_csi_driver.arn
  resolve_conflicts_on_update = "OVERWRITE"

  depends_on = [
    module.eks,
    aws_iam_role_policy_attachment.ebs_csi_driver
  ]

  tags = {
    Environment = var.environment
    ManagedBy   = "Terraform"
  }
}
```

ç„¶åè¿è¡Œï¼š

```bash
terraform apply
```

**æ–¹å¼ 2: æ‰‹åŠ¨å®‰è£…ï¼ˆå¦‚æœ CSI Driver å·²å­˜åœ¨ï¼‰**

å¦‚æœé›†ç¾¤ä¸­å·²ç»å­˜åœ¨ EBS CSI Driverï¼Œå¯ä»¥é€šè¿‡å˜é‡æ§åˆ¶ Terraform ä¸åˆ›å»ºï¼š

```hcl
variable "create_ebs_csi_driver" {
  description = "Whether to create EBS CSI Driver add-on"
  type        = bool
  default     = true
}

resource "aws_eks_addon" "ebs_csi_driver" {
  count = var.create_ebs_csi_driver ? 1 : 0
  # ... rest of configuration
}
```

**æ–¹å¼ 3: ä½¿ç”¨ kubectl æ‰‹åŠ¨å®‰è£…**

```bash
# å®‰è£… EBS CSI Driver
kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/?ref=release-1.32"

# æˆ–ä½¿ç”¨ Helm
helm repo add aws-ebs-csi-driver https://kubernetes-sigs.github.io/aws-ebs-csi-driver
helm install aws-ebs-csi-driver aws-ebs-csi-driver/aws-ebs-csi-driver \
  --namespace kube-system \
  --set controller.serviceAccount.create=false \
  --set controller.serviceAccount.name=ebs-csi-controller-sa
```

### éªŒè¯

```bash
# æ£€æŸ¥ EBS CSI Driver Pod çŠ¶æ€
kubectl get pods -n kube-system | grep ebs-csi

# åº”è¯¥çœ‹åˆ°ç±»ä¼¼è¾“å‡ºï¼š
# ebs-csi-controller-xxx   6/6     Running   0   5m
# ebs-csi-node-xxx          3/3     Running   0   5m

# æ£€æŸ¥ StorageClass
kubectl get storageclass

# åº”è¯¥çœ‹åˆ° gp3 æˆ– ebs-sc ç­‰ StorageClass

# æ£€æŸ¥ PVC çŠ¶æ€
kubectl get pvc -n monitoring

# ç­‰å¾…ä¸€æ®µæ—¶é—´åï¼ŒPVC åº”è¯¥ä» Pending å˜ä¸º Bound
```

---

## ğŸ” é—®é¢˜ 7: Prometheus - Grafana æ•°æ®æºé…ç½®å†²çª

### é”™è¯¯ä¿¡æ¯

```bash
kubectl logs -n monitoring prometheus-grafana-xxx -c grafana

Error: âœ— Datasource provisioning error: datasource.yaml config is invalid.
Only one datasource per organization can be marked as default
```

### åŸå› åˆ†æ

1. **å¤šä¸ªæ•°æ®æºé…ç½®å†²çª**ï¼škube-prometheus-stack ä¼šè‡ªåŠ¨åˆ›å»º `prometheus-kube-prometheus-grafana-datasource` ConfigMapï¼Œå…¶ä¸­åŒ…å« Prometheus (isDefault: true)
2. **é‡å¤é…ç½®**ï¼šåœ¨ `prometheus-values.yaml` ä¸­ä½¿ç”¨ `datasources` é…ç½®ä¼šåˆ›å»º `prometheus-grafana` ConfigMapï¼Œä¹ŸåŒ…å« Prometheus (isDefault: true)
3. **Grafana åŠ è½½å†²çª**ï¼šGrafana ä¼šåŠ è½½æ‰€æœ‰å¸¦æœ‰ `grafana_datasource: "1"` æ ‡ç­¾çš„ ConfigMapï¼Œå¯¼è‡´å¤šä¸ªé»˜è®¤æ•°æ®æºå†²çª

### è§£å†³æ–¹æ¡ˆ

**ä½¿ç”¨ `additionalDataSources` è€Œä¸æ˜¯ `datasources`**

ä¿®æ”¹ `monitoring/values/prometheus-values.yaml`ï¼š

```yaml
grafana:
  enabled: true
  # ... å…¶ä»–é…ç½® ...

  # âŒ é”™è¯¯æ–¹å¼ï¼šä¼šè¦†ç›–é»˜è®¤é…ç½®ï¼Œå¯¼è‡´å†²çª
  # datasources:
  #   datasources.yaml:
  #     apiVersion: 1
  #     datasources:
  #       - name: Prometheus
  #         isDefault: true
  #       - name: Loki
  #         isDefault: false

  # âœ… æ­£ç¡®æ–¹å¼ï¼šä½¿ç”¨ additionalDataSources æ·»åŠ é¢å¤–æ•°æ®æº
  additionalDataSources:
    - name: Loki
      type: loki
      access: proxy
      url: http://loki.monitoring.svc:3100
      isDefault: false # Prometheus å·²ç»ç”± kube-prometheus-stack è®¾ç½®ä¸ºé»˜è®¤
      editable: true
```

**å¦‚æœå·²ç»åˆ›å»ºäº†å†²çªçš„ ConfigMapï¼Œéœ€è¦åˆ é™¤ï¼š**

```bash
# åˆ é™¤å†²çªçš„ ConfigMapï¼ˆkube-prometheus-stack ä¼šè‡ªåŠ¨åˆ›å»ºæ­£ç¡®çš„ï¼‰
kubectl delete configmap prometheus-grafana -n monitoring

# åˆ é™¤ Grafana Pod è®©å®ƒé‡æ–°åŠ è½½é…ç½®
kubectl delete pod -n monitoring -l app.kubernetes.io/name=grafana

# ç­‰å¾… Pod é‡æ–°åˆ›å»º
kubectl get pods -n monitoring -l app.kubernetes.io/name=grafana -w
```

**æ‰‹åŠ¨ä¿®å¤ ConfigMapï¼ˆä¸´æ—¶æ–¹æ¡ˆï¼‰ï¼š**

```bash
# ç¼–è¾‘ ConfigMapï¼Œç¡®ä¿åªæœ‰ä¸€ä¸ª isDefault: true
kubectl edit configmap prometheus-kube-prometheus-grafana-datasource -n monitoring

# æˆ–ä½¿ç”¨ patch
kubectl patch configmap prometheus-kube-prometheus-grafana-datasource -n monitoring \
  --type='json' \
  -p='[{"op": "add", "path": "/data/datasource.yaml", "value": "apiVersion: 1\ndatasources:\n- name: Prometheus\n  isDefault: true\n- name: Loki\n  isDefault: false"}]'
```

### éªŒè¯

```bash
# æ£€æŸ¥æ•°æ®æº ConfigMap
kubectl get configmap -n monitoring -l grafana_datasource

# åº”è¯¥åªæœ‰ä¸€ä¸ª ConfigMapï¼šprometheus-kube-prometheus-grafana-datasource

# æ£€æŸ¥ ConfigMap å†…å®¹
kubectl get configmap prometheus-kube-prometheus-grafana-datasource -n monitoring -o yaml

# ç¡®ä¿åªæœ‰ä¸€ä¸ªæ•°æ®æºçš„ isDefault: true

# æ£€æŸ¥ Grafana Pod æ—¥å¿—
kubectl logs -n monitoring -l app.kubernetes.io/name=grafana -c grafana --tail=50

# åº”è¯¥æ²¡æœ‰æ•°æ®æºé…ç½®é”™è¯¯
```

---

## ğŸ” é—®é¢˜ 8: Grafana Pod ä¸€ç›´ Pending - èŠ‚ç‚¹èµ„æºä¸è¶³

### é”™è¯¯ä¿¡æ¯

```bash
kubectl describe pod -n monitoring prometheus-grafana-xxx

Events:
  Warning  FailedScheduling  0/2 nodes are available: 2 Too many pods.
  preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod.
```

### åŸå› åˆ†æ

1. **èŠ‚ç‚¹ Pod æ•°é‡é™åˆ¶**ï¼šæ¯ä¸ªèŠ‚ç‚¹æœ‰æœ€å¤§ Pod æ•°é‡é™åˆ¶ï¼ˆé€šå¸¸ç”± CNI å’ŒèŠ‚ç‚¹é…ç½®å†³å®šï¼‰
2. **èµ„æºè€—å°½**ï¼šèŠ‚ç‚¹ä¸Šå·²ç»è¿è¡Œäº†å¤ªå¤š Podï¼Œæ— æ³•è°ƒåº¦æ–°çš„ Pod
3. **é›†ç¾¤è§„æ¨¡ä¸è¶³**ï¼šå¯¹äºç›‘æ§æ ˆï¼ˆPrometheusã€Grafanaã€Loki ç­‰ï¼‰ï¼Œéœ€è¦è¶³å¤Ÿçš„èŠ‚ç‚¹èµ„æº

### è§£å†³æ–¹æ¡ˆ

**æ–¹å¼ 1: æ‰©å±•èŠ‚ç‚¹ï¼ˆæ¨èï¼‰**

```bash
# æ£€æŸ¥èŠ‚ç‚¹èµ„æº
kubectl get nodes -o json | jq -r '.items[] | "\(.metadata.name) pods: \(.status.allocatable.pods)"'

# æ£€æŸ¥å½“å‰è¿è¡Œçš„ Pod æ•°é‡
kubectl get pods --all-namespaces --field-selector=status.phase=Running --no-headers | wc -l

# å¦‚æœèŠ‚ç‚¹èµ„æºä¸è¶³ï¼Œéœ€è¦æ‰©å±• EKS èŠ‚ç‚¹ç»„
# åœ¨ Terraform ä¸­å¢åŠ èŠ‚ç‚¹æ•°é‡æˆ–èŠ‚ç‚¹ç±»å‹
```

**æ–¹å¼ 2: æ¸…ç†ä¸å¿…è¦çš„ Pod**

```bash
# æ£€æŸ¥æ‰€æœ‰å‘½åç©ºé—´çš„ Pod
kubectl get pods --all-namespaces

# åˆ é™¤ä¸å¿…è¦çš„ Pod æˆ–åº”ç”¨
# æ³¨æ„ï¼šåªåˆ é™¤ç¡®å®šä¸éœ€è¦çš„èµ„æº
```

**æ–¹å¼ 3: ç­‰å¾…å…¶ä»– Pod å®Œæˆ**

å¦‚æœæœ‰ä¸€äº› Job æˆ–ä¸´æ—¶ Pod æ­£åœ¨è¿è¡Œï¼Œç­‰å¾…å®ƒä»¬å®Œæˆåä¼šè‡ªåŠ¨é‡Šæ”¾èµ„æºã€‚

### éªŒè¯

```bash
# æ£€æŸ¥èŠ‚ç‚¹èµ„æº
kubectl top nodes  # å¦‚æœ metrics-server å·²å®‰è£…

# æ£€æŸ¥ Pod è°ƒåº¦çŠ¶æ€
kubectl get pods -n monitoring -o wide

# ç­‰å¾…èµ„æºé‡Šæ”¾åï¼ŒPending çš„ Pod åº”è¯¥ä¼šè‡ªåŠ¨è°ƒåº¦
```

---

## ğŸ” é—®é¢˜ 9: Loki - StatefulSet volumeClaimTemplates ç¼ºå°‘ storageClassName

### é”™è¯¯ä¿¡æ¯

```bash
kubectl get pvc -n monitoring | grep loki
# STATUS: Pending
# StorageClass: <unset>

kubectl describe pvc data-loki-backend-0 -n monitoring
# Events: no persistent volumes available for this claim and no storage class is set
```

### åŸå› åˆ†æ

1. **StatefulSet é™åˆ¶**ï¼šKubernetes StatefulSet çš„ `volumeClaimTemplates` å­—æ®µæ— æ³•ç›´æ¥ä¿®æ”¹ï¼ˆåªèƒ½ä¿®æ”¹ replicasã€templateã€updateStrategy ç­‰ï¼‰
2. **Helm Chart é…ç½®é—®é¢˜**ï¼šè™½ç„¶ values æ–‡ä»¶ä¸­é…ç½®äº† `simpleScalable.backend.persistence.storageClassName: gp3`ï¼Œä½† Helm Chart å¯èƒ½æ²¡æœ‰æ­£ç¡®åº”ç”¨åˆ° StatefulSet çš„ volumeClaimTemplates
3. **PVC æ— æ³•ç»‘å®š**ï¼šæ²¡æœ‰ storageClassName çš„ PVC æ— æ³•è¢« EBS CSI Driver åŠ¨æ€åˆ›å»º

### è§£å†³æ–¹æ¡ˆ

**æ–¹å¼ 1: æ‰‹åŠ¨ Patch ç°æœ‰ PVCï¼ˆä¸´æ—¶æ–¹æ¡ˆï¼‰**

```bash
# ä¸ºæ‰€æœ‰ Loki PVC æ·»åŠ  storageClassName
for pvc in $(kubectl get pvc -n monitoring -o name | grep loki); do
  kubectl patch $pvc -n monitoring --type='merge' -p '{"spec":{"storageClassName":"gp3"}}'
done

# æ£€æŸ¥ PVC çŠ¶æ€
kubectl get pvc -n monitoring | grep loki
```

**æ–¹å¼ 2: åˆ é™¤å¹¶é‡æ–°åˆ›å»º StatefulSetï¼ˆæ¨èï¼‰**

ç”±äº StatefulSet çš„ volumeClaimTemplates æ— æ³•ç›´æ¥ä¿®æ”¹ï¼Œéœ€è¦åˆ é™¤ StatefulSet è®© Helm é‡æ–°åˆ›å»ºï¼š

```bash
# åˆ é™¤ StatefulSetï¼ˆä¿ç•™ PVCï¼Œå› ä¸ºæ•°æ®å¯èƒ½é‡è¦ï¼‰
kubectl delete statefulset loki-backend loki-write -n monitoring --cascade=orphan

# è§¦å‘ ArgoCD é‡æ–°åŒæ­¥
kubectl annotate application loki -n argocd argocd.argoproj.io/refresh=hard --overwrite
kubectl patch application loki -n argocd --type merge -p '{"operation":{"initiatedBy":{"username":"admin"},"sync":{"revision":"main","prune":true}}}'

# ç­‰å¾… StatefulSet é‡æ–°åˆ›å»º
kubectl get statefulset -n monitoring | grep loki
```

**æ³¨æ„**ï¼šå¦‚æœ Helm Chart çš„é…ç½®è·¯å¾„ä¸æ­£ç¡®ï¼ŒStatefulSet é‡æ–°åˆ›å»ºåå¯èƒ½è¿˜æ˜¯æ²¡æœ‰ storageClassNameã€‚éœ€è¦æ£€æŸ¥ Helm Chart çš„æ–‡æ¡£ï¼Œç¡®è®¤æ­£ç¡®çš„é…ç½®è·¯å¾„ã€‚

**æ–¹å¼ 3: æ›´æ–° Helm Values æ–‡ä»¶**

ç¡®ä¿ `monitoring/values/loki-values-s3.yaml` ä¸­æ­£ç¡®é…ç½®äº† storageClassNameï¼š

```yaml
simpleScalable:
  backend:
    persistence:
      enabled: true
      storageClassName: gp3
      size: 10Gi
  write:
    persistence:
      enabled: true
      storageClassName: gp3
      size: 10Gi
```

### éªŒè¯

```bash
# æ£€æŸ¥ StatefulSet çš„ volumeClaimTemplates
kubectl get statefulset loki-backend -n monitoring -o jsonpath='{.spec.volumeClaimTemplates[0].spec.storageClassName}'
# åº”è¯¥è¾“å‡º: gp3

# æ£€æŸ¥ PVC çŠ¶æ€
kubectl get pvc -n monitoring | grep loki
# åº”è¯¥æ˜¾ç¤º storageClassName: gp3ï¼Œå¹¶ä¸” STATUS ä¸º Bound æˆ– Pendingï¼ˆç­‰å¾… Pod è°ƒåº¦ï¼‰

# æ£€æŸ¥ Pod çŠ¶æ€
kubectl get pods -n monitoring -l app.kubernetes.io/name=loki
```

---

## ğŸ” é—®é¢˜ 10: Loki Pod CrashLoopBackOff - Schema é…ç½®æœªç”Ÿæ•ˆ

### é”™è¯¯ä¿¡æ¯

```bash
kubectl logs -n monitoring loki-backend-0 -c loki

Error: CONFIG ERROR: schema v13 is required to store Structured Metadata and use native OTLP ingestion,
your schema version is v11. Set `allow_structured_metadata: false` in the `limits_config` section...
CONFIG ERROR: `tsdb` index type is required to store Structured Metadata and use native OTLP ingestion,
your index type is `boltdb-shipper`...
```

### åŸå› åˆ†æ

1. **é…ç½®æœªåŒæ­¥**ï¼šè™½ç„¶ values æ–‡ä»¶ä¸­å·²ç»æ›´æ–°ä¸º schema v13 å’Œ tsdbï¼Œä½† ArgoCD å¯èƒ½è¿˜æ²¡æœ‰åŒæ­¥ï¼Œæˆ–è€… ConfigMap è¿˜æ²¡æœ‰æ›´æ–°
2. **Loki ç‰ˆæœ¬è¦æ±‚**ï¼šLoki 3.0.0 ç‰ˆæœ¬è¦æ±‚ä½¿ç”¨ schema v13 å’Œ tsdb ç´¢å¼•ç±»å‹
3. **é…ç½®ç¼“å­˜**ï¼šHelm Chart å¯èƒ½ç¼“å­˜äº†æ—§çš„é…ç½®

### è§£å†³æ–¹æ¡ˆ

**æ­¥éª¤ 1: ç¡®è®¤ values æ–‡ä»¶é…ç½®æ­£ç¡®**

æ£€æŸ¥ `monitoring/values/loki-values-s3.yaml`ï¼š

```yaml
loki:
  schemaConfig:
    configs:
      - from: "2020-10-24"
        store: tsdb # å¿…é¡»æ˜¯ tsdbï¼Œä¸æ˜¯ boltdb-shipper
        object_store: s3
        schema: v13 # å¿…é¡»æ˜¯ v13ï¼Œä¸æ˜¯ v11
        index:
          prefix: index_
          period: 24h
  limits_config:
    allow_structured_metadata: false # å¿…é¡»è®¾ç½®ä¸º false
```

**æ­¥éª¤ 2: è§¦å‘ ArgoCD é‡æ–°åŒæ­¥**

```bash
# åˆ·æ–° ArgoCD Application
kubectl annotate application loki -n argocd argocd.argoproj.io/refresh=hard --overwrite

# æ‰‹åŠ¨è§¦å‘åŒæ­¥
kubectl patch application loki -n argocd --type merge -p '{"operation":{"initiatedBy":{"username":"admin"},"sync":{"revision":"main","prune":true}}}'

# ç­‰å¾…åŒæ­¥å®Œæˆ
kubectl get application loki -n argocd -w
```

**æ­¥éª¤ 3: æ£€æŸ¥ ConfigMap æ˜¯å¦æ›´æ–°**

```bash
# æ£€æŸ¥ Loki ConfigMap
kubectl get configmap loki -n monitoring -o yaml | grep -A 10 "schemaConfig:"

# åº”è¯¥çœ‹åˆ° store: tsdb å’Œ schema: v13
```

**æ­¥éª¤ 4: é‡å¯ Loki Pod**

```bash
# åˆ é™¤ Pod è®©å®ƒä»¬é‡æ–°åˆ›å»ºå¹¶åŠ è½½æ–°é…ç½®
kubectl delete pod -n monitoring -l app.kubernetes.io/name=loki

# ç­‰å¾… Pod é‡æ–°åˆ›å»º
kubectl get pods -n monitoring -l app.kubernetes.io/name=loki -w
```

### éªŒè¯

```bash
# æ£€æŸ¥ Pod æ—¥å¿—ï¼Œç¡®è®¤æ²¡æœ‰é…ç½®é”™è¯¯
kubectl logs -n monitoring loki-backend-0 -c loki --tail=50

# åº”è¯¥æ²¡æœ‰ schema æˆ– index type ç›¸å…³çš„é”™è¯¯

# æ£€æŸ¥ Pod çŠ¶æ€
kubectl get pods -n monitoring -l app.kubernetes.io/name=loki
# åº”è¯¥ä» CrashLoopBackOff å˜ä¸º Running
```

---

## ğŸ” é—®é¢˜ 11: Grafana Pod Pending - Volume Node Affinity Conflict

### é”™è¯¯ä¿¡æ¯

```bash
kubectl describe pod -n monitoring prometheus-grafana-xxx

Events:
  Warning  FailedScheduling  0/4 nodes are available: 1 node(s) had volume node affinity conflict
```

### åŸå› åˆ†æ

1. **EBS Volume Zone ç»‘å®š**ï¼šGrafana çš„ PVC å·²ç»ç»‘å®šåˆ°ç‰¹å®š Availability Zoneï¼ˆå¦‚ `ap-southeast-2c`ï¼‰
2. **èŠ‚ç‚¹åˆ†å¸ƒ**ï¼šé›†ç¾¤ä¸­çš„èŠ‚ç‚¹å¯èƒ½ä¸åœ¨è¯¥ zoneï¼Œæˆ–è€…è¯¥ zone çš„èŠ‚ç‚¹èµ„æºä¸è¶³
3. **WaitForFirstConsumer æ¨¡å¼**ï¼š`gp3` StorageClass ä½¿ç”¨ `WaitForFirstConsumer` æ¨¡å¼ï¼ŒPVC ä¼šåœ¨ Pod è°ƒåº¦åç»‘å®šï¼Œä½†å¦‚æœ PVC å·²ç»ç»‘å®šï¼ŒPod å¿…é¡»è°ƒåº¦åˆ°è¯¥ volume æ‰€åœ¨çš„ zone

### è§£å†³æ–¹æ¡ˆ

**æ–¹å¼ 1: æ£€æŸ¥èŠ‚ç‚¹ Zone åˆ†å¸ƒ**

```bash
# æŸ¥çœ‹æ‰€æœ‰èŠ‚ç‚¹çš„ zone
kubectl get nodes -o json | jq -r '.items[] | "\(.metadata.name) zone: \(.metadata.labels."topology.kubernetes.io/zone")"'

# æŸ¥çœ‹ Grafana PVC ç»‘å®šçš„ zone
kubectl get pv $(kubectl get pvc prometheus-grafana -n monitoring -o jsonpath='{.spec.volumeName}') -o jsonpath='{.spec.nodeAffinity.required.nodeSelectorTerms[0].matchExpressions[0].values[0]}'
```

**æ–¹å¼ 2: åˆ é™¤ Pod è®©è°ƒåº¦å™¨é‡æ–°è°ƒåº¦**

å¦‚æœ PVC ç»‘å®šçš„ zone æœ‰å¯ç”¨èŠ‚ç‚¹ï¼Œåˆ é™¤ Pod è®©å®ƒé‡æ–°è°ƒåº¦ï¼š

```bash
# åˆ é™¤ Grafana Pod
kubectl delete pod -n monitoring -l app.kubernetes.io/name=grafana

# ç­‰å¾… Pod é‡æ–°åˆ›å»ºå¹¶è°ƒåº¦
kubectl get pods -n monitoring -l app.kubernetes.io/name=grafana -w
```

**æ–¹å¼ 3: åˆ é™¤ PVC é‡æ–°åˆ›å»ºï¼ˆå¦‚æœæ•°æ®ä¸é‡è¦ï¼‰**

å¦‚æœ Grafana çš„æ•°æ®ä¸é‡è¦ï¼Œå¯ä»¥åˆ é™¤ PVC è®©è°ƒåº¦å™¨é‡æ–°åˆ›å»ºï¼š

```bash
# åˆ é™¤ Grafana Pod å’Œ PVC
kubectl delete pod -n monitoring -l app.kubernetes.io/name=grafana
kubectl delete pvc prometheus-grafana -n monitoring

# ç­‰å¾… Pod å’Œ PVC é‡æ–°åˆ›å»º
kubectl get pods -n monitoring -l app.kubernetes.io/name=grafana -w
kubectl get pvc -n monitoring | grep grafana
```

**æ–¹å¼ 4: æ‰©å±•èŠ‚ç‚¹åˆ° PVC æ‰€åœ¨çš„ Zone**

å¦‚æœ PVC ç»‘å®šçš„ zone æ²¡æœ‰èŠ‚ç‚¹æˆ–èŠ‚ç‚¹èµ„æºä¸è¶³ï¼Œå¯ä»¥æ‰©å±•èŠ‚ç‚¹ï¼š

```bash
# æ£€æŸ¥èŠ‚ç‚¹ç»„é…ç½®ï¼Œç¡®ä¿åœ¨ PVC æ‰€åœ¨çš„ zone æœ‰èŠ‚ç‚¹
# åœ¨ Terraform ä¸­é…ç½®å¤šä¸ª Availability Zone
```

### éªŒè¯

```bash
# æ£€æŸ¥ Pod è°ƒåº¦çŠ¶æ€
kubectl get pods -n monitoring -l app.kubernetes.io/name=grafana -o wide

# æ£€æŸ¥ Pod æ˜¯å¦è°ƒåº¦åˆ°æ­£ç¡®çš„èŠ‚ç‚¹ï¼ˆä¸ PVC åœ¨åŒä¸€ zoneï¼‰
kubectl get pod -n monitoring -l app.kubernetes.io/name=grafana -o jsonpath='{.items[0].spec.nodeName}'
kubectl get node <node-name> -o jsonpath='{.metadata.labels."topology.kubernetes.io/zone"}'
```

---

## ğŸ” é—®é¢˜ 12: èŠ‚ç‚¹èµ„æºä¸è¶³å¯¼è‡´ Pod æ— æ³•è°ƒåº¦

### é”™è¯¯ä¿¡æ¯

```bash
kubectl describe pod -n monitoring <pod-name>

Events:
  Warning  FailedScheduling  0/4 nodes are available: 4 Too many pods.
  preemption: 0/4 nodes are available: 4 No preemption victims found for incoming pod.
```

### åŸå› åˆ†æ

1. **èŠ‚ç‚¹ Pod æ•°é‡é™åˆ¶**ï¼šæ¯ä¸ªèŠ‚ç‚¹æœ‰æœ€å¤§ Pod æ•°é‡é™åˆ¶ï¼ˆé€šå¸¸ç”± CNI å’ŒèŠ‚ç‚¹é…ç½®å†³å®šï¼Œå¦‚ 17 ä¸ª Pod/èŠ‚ç‚¹ï¼‰
2. **èµ„æºè€—å°½**ï¼šèŠ‚ç‚¹ä¸Šå·²ç»è¿è¡Œäº†å¤ªå¤š Podï¼Œæ— æ³•è°ƒåº¦æ–°çš„ Pod
3. **ç›‘æ§æ ˆèµ„æºéœ€æ±‚**ï¼šç›‘æ§æ ˆï¼ˆPrometheusã€Grafanaã€Loki ç­‰ï¼‰éœ€è¦è¾ƒå¤šèµ„æº

### è§£å†³æ–¹æ¡ˆ

**æ–¹å¼ 1: æ‰©å±•èŠ‚ç‚¹æ•°é‡ï¼ˆæ¨èï¼‰**

```bash
# æ£€æŸ¥å½“å‰èŠ‚ç‚¹æ•°å’Œ Pod åˆ†å¸ƒ
kubectl get nodes -o json | jq -r '.items[] | "\(.metadata.name) pods: \(.status.allocatable.pods)"'
kubectl get pods --all-namespaces -o wide --field-selector=status.phase=Running --no-headers | awk '{print $8}' | sort | uniq -c

# ä½¿ç”¨ AWS CLI æ‰©å±•èŠ‚ç‚¹ç»„
aws eks update-nodegroup-config \
  --cluster-name eks-test \
  --nodegroup-name <nodegroup-name> \
  --scaling-config desiredSize=4,maxSize=4 \
  --region ap-southeast-2

# æˆ–ä½¿ç”¨ Terraform
# ä¿®æ”¹ terraform/variables.tf ä¸­çš„ node_desired_size å’Œ node_max_size
terraform apply
```

**æ–¹å¼ 2: ç­‰å¾…ä¸´æ—¶ Pod å®Œæˆ**

å¦‚æœæœ‰ä¸€äº› Job æˆ–ä¸´æ—¶ Pod æ­£åœ¨è¿è¡Œï¼Œç­‰å¾…å®ƒä»¬å®Œæˆåä¼šè‡ªåŠ¨é‡Šæ”¾èµ„æºï¼š

```bash
# æ£€æŸ¥ Job çŠ¶æ€
kubectl get jobs --all-namespaces

# ç­‰å¾… Job å®Œæˆ
kubectl wait --for=condition=complete job/<job-name> -n <namespace> --timeout=300s
```

**æ–¹å¼ 3: æ¸…ç†ä¸å¿…è¦çš„ Pod**

```bash
# æ£€æŸ¥æ‰€æœ‰å‘½åç©ºé—´çš„ Pod
kubectl get pods --all-namespaces

# åˆ é™¤ä¸å¿…è¦çš„ Pod æˆ–åº”ç”¨
# æ³¨æ„ï¼šåªåˆ é™¤ç¡®å®šä¸éœ€è¦çš„èµ„æº
```

### éªŒè¯

```bash
# æ£€æŸ¥èŠ‚ç‚¹èµ„æº
kubectl get nodes -o custom-columns=NAME:.metadata.name,PODS:.status.allocatable.pods

# æ£€æŸ¥å½“å‰è¿è¡Œçš„ Pod æ•°é‡
kubectl get pods --all-namespaces --field-selector=status.phase=Running --no-headers | wc -l

# æ£€æŸ¥ Pod è°ƒåº¦çŠ¶æ€
kubectl get pods -n monitoring --field-selector=status.phase=Pending -o wide

# ç­‰å¾…èµ„æºé‡Šæ”¾åï¼ŒPending çš„ Pod åº”è¯¥ä¼šè‡ªåŠ¨è°ƒåº¦
```

---

## ğŸ” é—®é¢˜ 13: Grafana Pod Pending - åˆ é™¤ PVC é‡æ–°åˆ›å»ºè§£å†³ Volume Node Affinity Conflict

### é—®é¢˜æè¿°

Grafana Pod ä¸€ç›´å¤„äº Pending çŠ¶æ€ï¼Œé”™è¯¯ä¿¡æ¯æ˜¾ç¤º `volume node affinity conflict`ã€‚

### åŸå› åˆ†æ

1. **PVC å·²ç»‘å®šåˆ°ç‰¹å®š Zone**ï¼šGrafana çš„ PVC å·²ç»ç»‘å®šåˆ°ç‰¹å®šçš„ Availability Zoneï¼ˆå¦‚ `ap-southeast-2c`ï¼‰
2. **èŠ‚ç‚¹èµ„æºä¸è¶³**ï¼šè¯¥ zone çš„èŠ‚ç‚¹èµ„æºä¸è¶³ï¼Œæ— æ³•è°ƒåº¦ Pod
3. **WaitForFirstConsumer æ¨¡å¼**ï¼š`gp3` StorageClass ä½¿ç”¨ `WaitForFirstConsumer` æ¨¡å¼ï¼Œä½†å¦‚æœ PVC å·²ç»ç»‘å®šï¼ŒPod å¿…é¡»è°ƒåº¦åˆ°è¯¥ volume æ‰€åœ¨çš„ zone

### è§£å†³æ–¹æ¡ˆ

**å¦‚æœ Grafana æ•°æ®ä¸é‡è¦ï¼Œåˆ é™¤ PVC è®©è°ƒåº¦å™¨é‡æ–°åˆ›å»ºï¼š**

```bash
# åˆ é™¤ Grafana Pod
kubectl delete pod -n monitoring -l app.kubernetes.io/name=grafana

# åˆ é™¤ Grafana PVCï¼ˆæ•°æ®ä¼šä¸¢å¤±ï¼Œä½†ä¼šé‡æ–°åˆ›å»ºï¼‰
kubectl delete pvc prometheus-grafana -n monitoring

# ç­‰å¾… Pod å’Œ PVC é‡æ–°åˆ›å»º
kubectl get pods -n monitoring -l app.kubernetes.io/name=grafana -w
kubectl get pvc -n monitoring | grep grafana
```

**æ³¨æ„**ï¼šåˆ é™¤ PVC ä¼šå¯¼è‡´ Grafana çš„æ•°æ®ï¼ˆä»ªè¡¨æ¿ã€ç”¨æˆ·é…ç½®ç­‰ï¼‰ä¸¢å¤±ã€‚å¦‚æœæ•°æ®é‡è¦ï¼Œåº”è¯¥ï¼š

1. å…ˆå¤‡ä»½æ•°æ®
2. æˆ–ç­‰å¾…èŠ‚ç‚¹èµ„æºé‡Šæ”¾
3. æˆ–æ‰©å±•èŠ‚ç‚¹åˆ° PVC æ‰€åœ¨çš„ zone

### éªŒè¯

```bash
# æ£€æŸ¥ Pod çŠ¶æ€
kubectl get pods -n monitoring -l app.kubernetes.io/name=grafana

# åº”è¯¥ä» Pending å˜ä¸º Running

# æ£€æŸ¥ PVC çŠ¶æ€
kubectl get pvc -n monitoring | grep grafana

# æ–°çš„ PVC åº”è¯¥å·²ç» Bound
```

---

## ğŸ” é—®é¢˜ 14: Loki Schema é…ç½®æ›´æ–°åéœ€è¦æäº¤åˆ° Git å¹¶é‡å¯ Pod

### é—®é¢˜æè¿°

è™½ç„¶æ›´æ–°äº† `loki-values-s3.yaml` æ–‡ä»¶ï¼ˆschema v13, tsdbï¼‰ï¼Œä½† Loki Pod ä»ç„¶æŠ¥é”™ï¼Œæ˜¾ç¤ºé…ç½®è¿˜æ˜¯ v11 å’Œ boltdb-shipperã€‚

### åŸå› åˆ†æ

1. **é…ç½®æœªæäº¤åˆ° Git**ï¼švalues æ–‡ä»¶çš„æ›´æ”¹åªå­˜åœ¨äºæœ¬åœ°ï¼Œæ²¡æœ‰æäº¤åˆ° Git ä»“åº“
2. **ArgoCD æœªåŒæ­¥**ï¼šArgoCD ä» Git ä»“åº“è¯»å–é…ç½®ï¼Œæœ¬åœ°æ›´æ”¹ä¸ä¼šè‡ªåŠ¨åŒæ­¥
3. **Pod ä½¿ç”¨æ—§é…ç½®**ï¼šå³ä½¿ ConfigMap æ›´æ–°äº†ï¼ŒPod å¯èƒ½è¿˜åœ¨ä½¿ç”¨æ—§çš„é…ç½®ç¼“å­˜

### è§£å†³æ–¹æ¡ˆ

**æ­¥éª¤ 1: æäº¤é…ç½®åˆ° Git**

```bash
# æ£€æŸ¥æœªæäº¤çš„æ›´æ”¹
git status monitoring/values/loki-values-s3.yaml

# æ·»åŠ å¹¶æäº¤æ›´æ”¹
git add monitoring/values/loki-values-s3.yaml
git commit -m "fix: Update Loki schema to v13 and tsdb, add storageClassName configuration"
git push origin main
```

**æ­¥éª¤ 2: è§¦å‘ ArgoCD åŒæ­¥**

```bash
# åˆ·æ–° ArgoCD Application
kubectl annotate application loki -n argocd argocd.argoproj.io/refresh=hard --overwrite

# æ‰‹åŠ¨è§¦å‘åŒæ­¥
kubectl patch application loki -n argocd --type merge -p '{"operation":{"initiatedBy":{"username":"admin"},"sync":{"revision":"main","prune":true}}}'

# ç­‰å¾…åŒæ­¥å®Œæˆ
kubectl get application loki -n argocd -w
```

**æ­¥éª¤ 3: éªŒè¯ ConfigMap å·²æ›´æ–°**

```bash
# æ£€æŸ¥ Loki ConfigMap
kubectl get configmap loki -n monitoring -o yaml | grep -A 3 "store:\|schema:"

# åº”è¯¥çœ‹åˆ° store: tsdb å’Œ schema: v13
```

**æ­¥éª¤ 4: é‡å¯ Loki Pod åŠ è½½æ–°é…ç½®**

```bash
# åˆ é™¤æ‰€æœ‰ Loki Pod è®©å®ƒä»¬é‡æ–°åˆ›å»ºå¹¶åŠ è½½æ–°é…ç½®
kubectl delete pod -n monitoring -l app.kubernetes.io/name=loki

# ç­‰å¾… Pod é‡æ–°åˆ›å»º
kubectl get pods -n monitoring -l app.kubernetes.io/name=loki -w
```

### éªŒè¯

```bash
# æ£€æŸ¥ Pod æ—¥å¿—ï¼Œç¡®è®¤æ²¡æœ‰é…ç½®é”™è¯¯
kubectl logs -n monitoring loki-backend-0 -c loki --tail=50

# åº”è¯¥æ²¡æœ‰ schema æˆ– index type ç›¸å…³çš„é”™è¯¯

# æ£€æŸ¥ Pod çŠ¶æ€
kubectl get pods -n monitoring -l app.kubernetes.io/name=loki
# åº”è¯¥ä» CrashLoopBackOff å˜ä¸º Running
```

---

## ğŸ” é—®é¢˜ 15: Loki S3 Bucket åç§°ä¸åŒ¹é…

### é—®é¢˜æè¿°

Loki ConfigMap ä¸­é…ç½®çš„ S3 bucket åç§°ä¸ Terraform å®é™…åˆ›å»ºçš„ bucket åç§°ä¸åŒ¹é…ï¼Œå¯¼è‡´ S3 è®¿é—®å¤±è´¥ã€‚

### é”™è¯¯ä¿¡æ¯

```bash
# æ£€æŸ¥ ConfigMap ä¸­çš„ bucket åç§°
kubectl get configmap loki -n monitoring -o yaml | grep bucketnames
# è¾“å‡º: bucketnames: eks-test-loki-storage-d6756e1c

# æ£€æŸ¥ Terraform è¾“å‡ºçš„ bucket åç§°
terraform output loki_s3_bucket_name
# è¾“å‡º: eks-test-loki-storage-565c7d68

# éªŒè¯ bucket æ˜¯å¦å­˜åœ¨
aws s3 ls s3://eks-test-loki-storage-d6756e1c/
# é”™è¯¯: NoSuchBucket
```

### åŸå› åˆ†æ

**æ ¹æœ¬åŸå› ï¼šè‡ªåŠ¨åŒ–è„šæœ¬ä¸æ–‡ä»¶æ ¼å¼ä¸åŒ¹é…ï¼Œå¯¼è‡´é…ç½®æ— æ³•è‡ªåŠ¨åŒæ­¥**

1. **Terraform ä½¿ç”¨éšæœºåç¼€ç”Ÿæˆ bucket åç§°**

   - Terraform é…ç½®ä½¿ç”¨ `random_id.bucket_suffix.hex` ç”Ÿæˆ 8 ä½éšæœºåå…­è¿›åˆ¶åç¼€
   - æ ¼å¼ï¼š`${cluster_name}-loki-storage-${random_id.bucket_suffix.hex}`
   - æ¯æ¬¡ `terraform apply`ï¼ˆç‰¹åˆ«æ˜¯ destroy åé‡æ–°åˆ›å»ºï¼‰å¯èƒ½ç”Ÿæˆä¸åŒçš„åç¼€
   - ä¾‹å¦‚ï¼š`eks-test-loki-storage-d6756e1c` -> `eks-test-loki-storage-565c7d68`

2. **è‡ªåŠ¨åŒ–è„šæœ¬æœŸæœ›çš„æ ¼å¼ä¸å®é™…æ–‡ä»¶ä¸åŒ¹é…**

   - `terraform/update-loki-values.sh` è„šæœ¬æœŸæœ›åœ¨ `loki-values-s3.yaml` ä¸­æ‰¾åˆ°å ä½ç¬¦ï¼š
     - `${LOKI_S3_BUCKET_NAME}`
     - `${AWS_REGION}`
   - è„šæœ¬ä½¿ç”¨ `sed` å‘½ä»¤æ›¿æ¢è¿™äº›å ä½ç¬¦ï¼š
     ```bash
     sed -e "s|\${LOKI_S3_BUCKET_NAME}|${BUCKET_NAME}|g" \
         -e "s|\${AWS_REGION}|${AWS_REGION}|g" \
         "${VALUES_FILE}"
     ```
   - ä½†å®é™…æ–‡ä»¶ä¸­æ˜¯ç¡¬ç¼–ç çš„ bucket åç§°ï¼š`eks-test-loki-storage-d6756e1c`
   - è„šæœ¬æ‰¾ä¸åˆ°å ä½ç¬¦ï¼Œæ— æ³•è¿›è¡Œæ›¿æ¢ï¼Œå¯¼è‡´é…ç½®æ— æ³•è‡ªåŠ¨æ›´æ–°

3. **é…ç½®åŒæ­¥ç¼ºå¤±**

   - å½“ Terraform é‡æ–°åˆ›å»ºèµ„æºæ—¶ï¼ˆå¦‚ `terraform destroy` å `terraform apply`ï¼‰ï¼Œbucket åç§°ä¼šå˜åŒ–
   - ä½† `loki-values-s3.yaml` æ–‡ä»¶ä¸­çš„ bucket åç§°æ²¡æœ‰åŒæ­¥æ›´æ–°
   - å¦‚æœè„šæœ¬æ²¡æœ‰è¿è¡Œï¼Œæˆ–è€…è„šæœ¬æ— æ³•æ‰¾åˆ°å ä½ç¬¦ï¼Œé…ç½®å°±ä¼šä¿æŒæ—§å€¼
   - å¯¼è‡´ Loki å°è¯•è®¿é—®ä¸å­˜åœ¨çš„ bucketï¼ˆæ—§çš„ bucket åç§°ï¼‰

4. **å·¥ä½œæµç¨‹é—®é¢˜**
   - è„šæœ¬åº”è¯¥åœ¨ `terraform apply` åè‡ªåŠ¨è¿è¡Œï¼Œä½†å¯èƒ½è¢«é—æ¼
   - æˆ–è€…æ–‡ä»¶åº”è¯¥ä½¿ç”¨å ä½ç¬¦è€Œä¸æ˜¯ç¡¬ç¼–ç å€¼
   - æˆ–è€…åº”è¯¥ä½¿ç”¨ Terraform çš„ `local_file` èµ„æºè‡ªåŠ¨ç”Ÿæˆ values æ–‡ä»¶

### è§£å†³æ–¹æ¡ˆ

**æ­¥éª¤ 1: è·å–æ­£ç¡®çš„ bucket åç§°**

```bash
# ä» Terraform è¾“å‡ºè·å–
cd terraform
terraform output loki_s3_bucket_name

# æˆ–ä» AWS ç›´æ¥æŸ¥çœ‹
aws s3 ls | grep loki
```

**æ­¥éª¤ 2: æ›´æ–° values æ–‡ä»¶**

ç¼–è¾‘ `monitoring/values/loki-values-s3.yaml`ï¼š

```yaml
loki:
  storage:
    bucketNames:
      chunks: eks-test-loki-storage-565c7d68 # ä½¿ç”¨æ­£ç¡®çš„ bucket åç§°
      ruler: eks-test-loki-storage-565c7d68 # ä½¿ç”¨æ­£ç¡®çš„ bucket åç§°
```

**æ­¥éª¤ 3: æäº¤åˆ° Git å¹¶åŒæ­¥**

```bash
# æäº¤æ›´æ”¹
git add monitoring/values/loki-values-s3.yaml
git commit -m "fix: Update Loki S3 bucket name to match Terraform output"
git push origin main

# è§¦å‘ ArgoCD åŒæ­¥
kubectl annotate application loki -n argocd argocd.argoproj.io/refresh=hard --overwrite
kubectl patch application loki -n argocd --type merge -p '{"operation":{"initiatedBy":{"username":"admin"},"sync":{"revision":"main","prune":true}}}'
```

**æ­¥éª¤ 4: éªŒè¯ ConfigMap å·²æ›´æ–°**

```bash
# æ£€æŸ¥ ConfigMap
kubectl get configmap loki -n monitoring -o yaml | grep -A 2 "bucketnames:"

# åº”è¯¥æ˜¾ç¤ºæ­£ç¡®çš„ bucket åç§°
```

**æ­¥éª¤ 5: é‡å¯ Loki Pod**

```bash
# åˆ é™¤ Pod è®©å®ƒä»¬é‡æ–°åŠ è½½é…ç½®
kubectl delete pod -n monitoring -l app.kubernetes.io/name=loki
```

### éªŒè¯

```bash
# æ£€æŸ¥ ConfigMap ä¸­çš„ bucket åç§°
kubectl get configmap loki -n monitoring -o yaml | grep bucketnames

# åº”è¯¥æ˜¾ç¤ºæ­£ç¡®çš„ bucket åç§°ï¼ˆä¸ Terraform è¾“å‡ºä¸€è‡´ï¼‰

# æ£€æŸ¥ Pod æ—¥å¿—ï¼Œç¡®è®¤æ²¡æœ‰ bucket ç›¸å…³çš„é”™è¯¯
kubectl logs -n monitoring loki-backend-0 -c loki --tail=50 | grep -i bucket
```

---

## ğŸ” é—®é¢˜ 16: Loki S3 è®¿é—®é”™è¯¯ - MethodNotAllowed

### é”™è¯¯ä¿¡æ¯

```bash
kubectl logs -n monitoring loki-backend-0 -c loki

level=error msg="sync failed, retrying it" err="WebIdentityErr: failed to retrieve credentials
caused by: SerializationError: failed to unmarshal error message
    status code: 405, request id:
caused by: UnmarshalError: failed to unmarshal error message
    <Error><Code>MethodNotAllowed</Code><Message>The specified method is not allowed against this resource.</Message>
    <Method>POST</Method><ResourceType>SERVICE</ResourceType>
```

### åŸå› åˆ†æ

1. **STS (Security Token Service) è°ƒç”¨é—®é¢˜**ï¼šé”™è¯¯ä¿¡æ¯æ˜¾ç¤º `ResourceType: SERVICE` å’Œ `Method: POST`ï¼Œè¡¨æ˜è¿™æ˜¯ STS è°ƒç”¨é—®é¢˜ï¼Œè€Œä¸æ˜¯ç›´æ¥çš„ S3 è®¿é—®é—®é¢˜
2. **IRSA é…ç½®**ï¼šè™½ç„¶æ—¥å¿—ä¸­æœ‰é”™è¯¯ï¼Œä½† IRSA é…ç½®çœ‹èµ·æ¥æ˜¯æ­£ç¡®çš„ï¼ˆServiceAccount æœ‰æ­£ç¡®çš„ IAM Role æ³¨è§£ï¼ŒIAM Role æœ‰æ­£ç¡®çš„ä¿¡ä»»ç­–ç•¥å’Œæƒé™ï¼‰
3. **AWS SDK è¡Œä¸º**ï¼šå¯èƒ½æ˜¯ AWS SDK åœ¨å°è¯•æŸäº›æ“ä½œæ—¶ä½¿ç”¨äº†é”™è¯¯çš„æ–¹æ³•ï¼Œä½†é‡è¯•åæˆåŠŸ
4. **åŠŸèƒ½å½±å“**ï¼šè™½ç„¶æ—¥å¿—ä¸­æœ‰é”™è¯¯ï¼Œä½† Loki çš„åŸºæœ¬åŠŸèƒ½ï¼ˆå†™å…¥å’ŒæŸ¥è¯¢ï¼‰æ˜¯æ­£å¸¸çš„

### æ’æŸ¥æ­¥éª¤

**æ­¥éª¤ 1: æ£€æŸ¥ ServiceAccount å’Œ IAM Role**

```bash
# æ£€æŸ¥ ServiceAccount
kubectl get serviceaccount loki-s3-service-account -n monitoring -o yaml

# æ£€æŸ¥ IAM Role ARN
kubectl get serviceaccount loki-s3-service-account -n monitoring -o jsonpath='{.metadata.annotations.eks\.amazonaws\.com/role-arn}'

# æ£€æŸ¥ IAM Role ä¿¡ä»»ç­–ç•¥
aws iam get-role --role-name eks-test-loki-s3-role --query 'Role.AssumeRolePolicyDocument' --output json
```

**æ­¥éª¤ 2: æ£€æŸ¥ S3 Bucket é…ç½®**

```bash
# æ£€æŸ¥ Loki ConfigMap ä¸­çš„ S3 bucket é…ç½®
kubectl get configmap loki -n monitoring -o yaml | grep -A 5 "bucketnames:"

# éªŒè¯ S3 bucket æ˜¯å¦å­˜åœ¨
aws s3 ls | grep loki
aws s3api head-bucket --bucket eks-test-loki-storage-565c7d68
```

**æ­¥éª¤ 3: æ£€æŸ¥ IAM Role æƒé™**

```bash
# è·å– IAM Role åç§°
ROLE_ARN=$(kubectl get serviceaccount loki-s3-service-account -n monitoring -o jsonpath='{.metadata.annotations.eks\.amazonaws\.com/role-arn}')
ROLE_NAME=$(echo $ROLE_ARN | awk -F'/' '{print $2}')

# æ£€æŸ¥ IAM Role çš„ç­–ç•¥
aws iam list-attached-role-policies --role-name $ROLE_NAME
aws iam get-policy-version --policy-arn arn:aws:iam::ACCOUNT:policy/POLICY_NAME --version-id VERSION_ID
```

**æ­¥éª¤ 4: éªŒè¯ Loki åŠŸèƒ½**

è™½ç„¶æ—¥å¿—ä¸­æœ‰é”™è¯¯ï¼Œä½†éœ€è¦éªŒè¯ Loki æ˜¯å¦çœŸçš„å¯ä»¥æ­£å¸¸å·¥ä½œï¼š

```bash
# æµ‹è¯•å†™å…¥æ•°æ®
kubectl exec -n monitoring loki-gateway-64c9b8cc4d-rctp7 -- wget -qO- --post-data='{"streams":[{"stream":{"job":"test"},"values":[["'$(date +%s)000000000'","test message"]]}]}' --header='Content-Type: application/json' http://localhost:8080/loki/api/v1/push

# æµ‹è¯•æŸ¥è¯¢æ•°æ®
kubectl exec -n monitoring loki-gateway-64c9b8cc4d-rctp7 -- wget -qO- 'http://localhost:8080/loki/api/v1/query?query={job="test"}'

# æ£€æŸ¥æ—¥å¿—ä¸­çš„æˆåŠŸæ“ä½œ
kubectl logs -n monitoring loki-backend-0 -c loki --tail=200 | grep -E "(downloaded|uploaded|success)"
```

### å®é™…æµ‹è¯•ç»“æœ

ç»è¿‡æµ‹è¯•ï¼Œå‘ç°ï¼š

1. âœ… **Loki å¯ä»¥æˆåŠŸå†™å…¥æ•°æ®**ï¼šPOST è¯·æ±‚æˆåŠŸï¼Œæ²¡æœ‰é”™è¯¯
2. âœ… **Loki å¯ä»¥æˆåŠŸæŸ¥è¯¢æ•°æ®**ï¼šæŸ¥è¯¢è¿”å›äº†åˆšæ‰å†™å…¥çš„æµ‹è¯•æ¶ˆæ¯
3. âœ… **æ—¥å¿—ä¸­æœ‰æˆåŠŸæ“ä½œ**ï¼š`downloaded index set at query time` è¡¨æ˜æŸäº›æ“ä½œæ˜¯æˆåŠŸçš„
4. âš ï¸ **æ—¥å¿—ä¸­ä»æœ‰é”™è¯¯**ï¼š`MethodNotAllowed` é”™è¯¯ä»ç„¶å­˜åœ¨ï¼Œä½†ä¼¼ä¹ä¸å½±å“åŸºæœ¬åŠŸèƒ½

### ç»“è®º

è™½ç„¶æ—¥å¿—ä¸­æœ‰ `MethodNotAllowed` é”™è¯¯ï¼Œä½† Loki çš„åŸºæœ¬åŠŸèƒ½ï¼ˆå†™å…¥å’ŒæŸ¥è¯¢ï¼‰æ˜¯æ­£å¸¸çš„ã€‚è¿™äº›é”™è¯¯å¯èƒ½æ˜¯ï¼š

1. **æŸäº›ç‰¹å®šæ“ä½œå¤±è´¥**ï¼šå¦‚ index è¡¨çš„åˆå§‹åŒ–æ“ä½œå¤±è´¥ï¼Œä½†ä¸å½±å“åŸºæœ¬åŠŸèƒ½
2. **AWS SDK è¡Œä¸º**ï¼šAWS SDK åœ¨å°è¯•æŸäº›æ“ä½œæ—¶ä½¿ç”¨äº†é”™è¯¯çš„æ–¹æ³•ï¼Œä½†é‡è¯•åæˆåŠŸ
3. **ä¸å½±å“æ ¸å¿ƒåŠŸèƒ½**ï¼šå†™å…¥å’ŒæŸ¥è¯¢åŠŸèƒ½æ­£å¸¸ï¼Œè¯´æ˜ S3 è®¿é—®æƒé™æ˜¯è¶³å¤Ÿçš„

### å»ºè®®

1. **ç»§ç»­è§‚å¯Ÿ**ï¼šå¦‚æœ Loki åŠŸèƒ½æ­£å¸¸ï¼Œå¯ä»¥æš‚æ—¶å¿½ç•¥è¿™äº›é”™è¯¯æ—¥å¿—
2. **ç›‘æ§åŠŸèƒ½**ï¼šå®šæœŸæ£€æŸ¥ Loki æ˜¯å¦çœŸçš„åœ¨å†™å…¥å’ŒæŸ¥è¯¢æ•°æ®
3. **å‡çº§ Loki ç‰ˆæœ¬**ï¼šå¦‚æœé—®é¢˜æŒç»­ï¼Œå¯ä»¥è€ƒè™‘å‡çº§ Loki ç‰ˆæœ¬ï¼Œå¯èƒ½ä¿®å¤äº†æŸäº› AWS SDK ç›¸å…³çš„é—®é¢˜
4. **æ£€æŸ¥ AWS SDK ç‰ˆæœ¬**ï¼šæŸäº› AWS SDK ç‰ˆæœ¬å¯èƒ½æœ‰å·²çŸ¥çš„ STS endpoint é—®é¢˜

### éªŒè¯

```bash
# æ£€æŸ¥ Loki Pod çŠ¶æ€
kubectl get pods -n monitoring -l app.kubernetes.io/name=loki

# åº”è¯¥å¤§éƒ¨åˆ† Pod éƒ½æ˜¯ Running çŠ¶æ€

# æµ‹è¯• Loki åŠŸèƒ½
kubectl exec -n monitoring loki-gateway-64c9b8cc4d-rctp7 -- wget -qO- http://localhost:8080/loki/api/v1/labels

# åº”è¯¥è¿”å›æ ‡ç­¾åˆ—è¡¨ï¼Œè¯´æ˜ Loki åŠŸèƒ½æ­£å¸¸
```

---

## ğŸ” é—®é¢˜ 16: loki-chunks-cache-0 Pod ä¸€ç›´ Pending

### é”™è¯¯ä¿¡æ¯

```bash
kubectl describe pod -n monitoring loki-chunks-cache-0

Events:
  Warning  FailedScheduling  0/4 nodes are available: 4 Insufficient memory, 4 Too many pods.
```

### åŸå› åˆ†æ

1. **èŠ‚ç‚¹èµ„æºä¸è¶³**ï¼šæ‰€æœ‰èŠ‚ç‚¹éƒ½èµ„æºä¸è¶³ï¼ˆå†…å­˜å’Œ Pod æ•°é‡ï¼‰
2. **èµ„æºéœ€æ±‚è¾ƒå¤§**ï¼š`loki-chunks-cache-0` éœ€è¦è¾ƒå¤šå†…å­˜èµ„æº
3. **èŠ‚ç‚¹æ•°é‡ä¸è¶³**ï¼šå³ä½¿æ‰©å±•åˆ° 4 ä¸ªèŠ‚ç‚¹ï¼Œç›‘æ§æ ˆçš„èµ„æºéœ€æ±‚ä»ç„¶å¾ˆå¤§

### è§£å†³æ–¹æ¡ˆ

**æ–¹å¼ 1: ç­‰å¾…èµ„æºé‡Šæ”¾**

å¦‚æœæœ‰ä¸€äº›ä¸´æ—¶ Pod æˆ– Job æ­£åœ¨è¿è¡Œï¼Œç­‰å¾…å®ƒä»¬å®Œæˆåä¼šè‡ªåŠ¨é‡Šæ”¾èµ„æºï¼š

```bash
# æ£€æŸ¥ Job çŠ¶æ€
kubectl get jobs --all-namespaces

# ç­‰å¾… Job å®Œæˆ
kubectl wait --for=condition=complete job/<job-name> -n <namespace> --timeout=300s
```

**æ–¹å¼ 2: æ‰©å±•èŠ‚ç‚¹ï¼ˆå¦‚æœèµ„æºæŒç»­ä¸è¶³ï¼‰**

```bash
# ç»§ç»­æ‰©å±•èŠ‚ç‚¹æ•°é‡
aws eks update-nodegroup-config \
  --cluster-name eks-test \
  --nodegroup-name <nodegroup-name> \
  --scaling-config desiredSize=5,maxSize=5 \
  --region ap-southeast-2
```

**æ–¹å¼ 3: å‡å°‘ loki-chunks-cache çš„èµ„æºè¯·æ±‚ï¼ˆæ¨èï¼‰**

å¦‚æœå¸Œæœ›ä¿ç•™ chunks-cache ä½†å‡å°‘èµ„æºéœ€æ±‚ï¼Œå¯ä»¥åœ¨ values æ–‡ä»¶ä¸­é…ç½®æ›´å°çš„èµ„æºè¯·æ±‚ï¼š

åœ¨ `monitoring/values/loki-values-s3.yaml` ä¸­æ·»åŠ ï¼š

```yaml
# Cache components configuration
# Reduce resource requests to fit within node capacity
chunksCache:
  enabled: true
  resources:
    requests:
      cpu: 500m
      memory: 1Gi # Reduced from default 9830Mi to fit node capacity (~3.8GB)
    limits:
      memory: 2Gi # Allow some burst but limit to prevent OOM

resultsCache:
  enabled: true
  resources:
    requests:
      cpu: 100m
      memory: 512Mi
    limits:
      memory: 1Gi
```

**æ­¥éª¤ï¼š**

1. æ›´æ–° values æ–‡ä»¶å¹¶æäº¤åˆ° Git
2. è§¦å‘ ArgoCD åŒæ­¥ï¼š
   ```bash
   kubectl annotate application loki -n argocd argocd.argoproj.io/refresh=hard --overwrite
   kubectl patch application loki -n argocd --type merge -p '{"operation":{"initiatedBy":{"username":"admin"},"sync":{"revision":"main","prune":true}}}'
   ```
3. ç­‰å¾… StatefulSet æ›´æ–°å¹¶ Pod é‡æ–°åˆ›å»º
4. éªŒè¯ Pod çŠ¶æ€ï¼š
   ```bash
   kubectl get pods -n monitoring loki-chunks-cache-0
   ```

**æ–¹å¼ 4: ç¦ç”¨ loki-chunks-cacheï¼ˆå¦‚æœä¸éœ€è¦ï¼‰**

å¦‚æœ chunks-cache ä¸æ˜¯å¿…éœ€çš„ï¼Œå¯ä»¥åœ¨ values æ–‡ä»¶ä¸­ç¦ç”¨å®ƒï¼š

```yaml
chunksCache:
  enabled: false
```

### éªŒè¯

```bash
# æ£€æŸ¥ Pod è°ƒåº¦çŠ¶æ€
kubectl get pods -n monitoring loki-chunks-cache-0 -o wide

# åº”è¯¥æ˜¾ç¤º Running çŠ¶æ€ï¼Œå¹¶ä¸”å·²è°ƒåº¦åˆ°æŸä¸ªèŠ‚ç‚¹

# æ£€æŸ¥èµ„æºè¯·æ±‚æ˜¯å¦å·²æ›´æ–°
kubectl get statefulset -n monitoring loki-chunks-cache -o jsonpath='{.spec.template.spec.containers[0].resources}' | jq .

# åº”è¯¥æ˜¾ç¤ºï¼š
# {
#   "limits": {
#     "memory": "2Gi"
#   },
#   "requests": {
#     "cpu": "500m",
#     "memory": "1Gi"
#   }
# }

# æ£€æŸ¥æ‰€æœ‰ Loki Pod çŠ¶æ€
kubectl get pods -n monitoring -l app.kubernetes.io/name=loki

# åº”è¯¥æ‰€æœ‰ Pod éƒ½æ˜¯ Running çŠ¶æ€
```

### å®é™…ç»“æœ

é€šè¿‡å‡å°‘èµ„æºè¯·æ±‚æˆåŠŸè§£å†³äº†é—®é¢˜ï¼š

- âœ… **èµ„æºè¯·æ±‚å·²æ›´æ–°**ï¼šä» 9830Mi å‡å°‘åˆ° 1Gi
- âœ… **Pod æˆåŠŸè°ƒåº¦**ï¼š`loki-chunks-cache-0` ç°åœ¨ Running çŠ¶æ€
- âœ… **ä¿ç•™äº†ç¼“å­˜åŠŸèƒ½**ï¼šchunksCache ä»ç„¶å¯ç”¨ï¼Œåªæ˜¯ä½¿ç”¨æ›´å°‘çš„èµ„æº
- âœ… **æ‰€æœ‰ Loki Pod æ­£å¸¸è¿è¡Œ**ï¼š16 ä¸ª Loki Pod éƒ½åœ¨ Running çŠ¶æ€

**ä¼˜åŠ¿ï¼š**

- ä¿ç•™äº† chunksCache çš„ç¼“å­˜åŠŸèƒ½ï¼Œæé«˜æŸ¥è¯¢æ€§èƒ½
- ä¸éœ€è¦å¢åŠ èŠ‚ç‚¹æˆ–å‡çº§èŠ‚ç‚¹è§„æ ¼
- ä¸éœ€è¦å®Œå…¨ç¦ç”¨ç¼“å­˜ç»„ä»¶

---

## ğŸ“š é™„å½•ï¼šLoki chunksCache è¯¦è§£

### chunksCache çš„ä½œç”¨

`chunksCache` æ˜¯ Loki çš„**å—ç¼“å­˜ç»„ä»¶**ï¼Œç”¨äºæé«˜æŸ¥è¯¢æ€§èƒ½ï¼š

1. **ç¼“å­˜æ—¥å¿—æ•°æ®å—ï¼ˆChunksï¼‰**

   - Loki å°†æ—¥å¿—æ•°æ®å­˜å‚¨åœ¨ S3 ç­‰å¯¹è±¡å­˜å‚¨ä¸­çš„"å—"ï¼ˆchunksï¼‰
   - chunksCache ç¼“å­˜è¿™äº›å—ï¼Œé¿å…æ¯æ¬¡æŸ¥è¯¢éƒ½ä» S3 è¯»å–
   - æ˜¾è‘—å‡å°‘å¯¹åç«¯å­˜å‚¨çš„è®¿é—®é¢‘ç‡

2. **åŠ é€ŸæŸ¥è¯¢å“åº”**

   - ç¼“å­˜å¸¸ç”¨çš„æ—¥å¿—å—ï¼Œä½¿æŸ¥è¯¢æ›´å¿«
   - ç‰¹åˆ«æ˜¯åœ¨é«˜å¹¶å‘æŸ¥è¯¢åœºæ™¯ä¸‹ï¼Œæ•ˆæœæ˜æ˜¾
   - å‡å°‘ç½‘ç»œå»¶è¿Ÿå’Œå­˜å‚¨ I/O

3. **é™ä½å­˜å‚¨æˆæœ¬**
   - å‡å°‘å¯¹ S3 çš„ API è°ƒç”¨æ¬¡æ•°
   - é™ä½æ•°æ®ä¼ è¾“æˆæœ¬

### ä¸ºä»€ä¹ˆé»˜è®¤éœ€è¦å¤§å†…å­˜ï¼ˆ9830Mi â‰ˆ 9.6GBï¼‰ï¼Ÿ

**åŸå› ï¼šMemcached é»˜è®¤é…ç½®éœ€è¦ 8GB å†…å­˜**

ä» StatefulSet é…ç½®å¯ä»¥çœ‹åˆ°ï¼ŒchunksCache ä½¿ç”¨ **Memcached** ä½œä¸ºç¼“å­˜åç«¯ï¼š

```yaml
containers:
  - name: memcached
    image: memcached:1.6.23-alpine
    args:
      - -m 8192 # åˆ†é… 8192MB (8GB) å†…å­˜ç»™ Memcached
      - --extended=modern,track_sizes
      - -I 5m # æœ€å¤§ item å¤§å° 5MB
      - -c 16384 # æœ€å¤§è¿æ¥æ•° 16384
```

**å†…å­˜åˆ†é…è¯´æ˜ï¼š**

1. **Memcached å†…å­˜é™åˆ¶**ï¼š`-m 8192` è¡¨ç¤º Memcached å¯ä»¥ä½¿ç”¨æœ€å¤š 8GB å†…å­˜
2. **Kubernetes èµ„æºè¯·æ±‚**ï¼š9830Miï¼ˆçº¦ 9.6GBï¼‰åŒ…æ‹¬ï¼š

   - Memcached çš„ 8GB å†…å­˜
   - æ“ä½œç³»ç»Ÿå’Œå…¶ä»–è¿›ç¨‹çš„é¢å¤–å†…å­˜ï¼ˆçº¦ 1.6GBï¼‰
   - å®‰å…¨ä½™é‡ï¼Œé˜²æ­¢ OOMï¼ˆOut of Memoryï¼‰

3. **ä¸ºä»€ä¹ˆéœ€è¦è¿™ä¹ˆå¤§ï¼Ÿ**
   - **ç”Ÿäº§ç¯å¢ƒè€ƒè™‘**ï¼šåœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå¯èƒ½æœ‰å¤§é‡çš„æ—¥å¿—æ•°æ®éœ€è¦ç¼“å­˜
   - **æ€§èƒ½ä¼˜åŒ–**ï¼šæ›´å¤§çš„ç¼“å­˜å¯ä»¥å­˜å‚¨æ›´å¤šçš„æ•°æ®å—ï¼Œå‡å°‘ç¼“å­˜æœªå‘½ä¸­
   - **é«˜å¹¶å‘åœºæ™¯**ï¼šæ”¯æŒæ›´å¤šçš„å¹¶å‘æŸ¥è¯¢å’Œè¿æ¥

### å‡å°‘èµ„æºåçš„å½±å“

å½“æˆ‘ä»¬å°†å†…å­˜è¯·æ±‚ä» 9830Mi å‡å°‘åˆ° 1Gi æ—¶ï¼š

1. **Memcached å†…å­˜é™åˆ¶ä¼šç›¸åº”è°ƒæ•´**

   - å®é™…å¯ç”¨çš„ç¼“å­˜å†…å­˜ä¼šå‡å°‘ï¼ˆå¯èƒ½åªæœ‰å‡ ç™¾ MBï¼‰
   - ç¼“å­˜å®¹é‡å˜å°ï¼Œç¼“å­˜æœªå‘½ä¸­ç‡å¯èƒ½å¢åŠ 

2. **æ€§èƒ½å½±å“**

   - âœ… **å°è§„æ¨¡åœºæ™¯**ï¼šå½±å“ä¸å¤§ï¼Œä»ç„¶æœ‰ç¼“å­˜æ•ˆæœ
   - âš ï¸ **å¤§è§„æ¨¡åœºæ™¯**ï¼šå¯èƒ½å½±å“æŸ¥è¯¢æ€§èƒ½ï¼Œéœ€è¦æ›´å¤šä» S3 è¯»å–
   - âš ï¸ **é«˜å¹¶å‘åœºæ™¯**ï¼šç¼“å­˜å¯èƒ½ä¸å¤Ÿç”¨ï¼Œæ€§èƒ½ä¸‹é™

3. **å»ºè®®**
   - **æµ‹è¯•/å¼€å‘ç¯å¢ƒ**ï¼š1-2Gi è¶³å¤Ÿ
   - **å°è§„æ¨¡ç”Ÿäº§ç¯å¢ƒ**ï¼š2-4Gi å¯ä»¥æ¥å—
   - **å¤§è§„æ¨¡ç”Ÿäº§ç¯å¢ƒ**ï¼šå»ºè®®ä¿æŒ 8GB æˆ–æ›´å¤šï¼Œå¹¶ç›¸åº”å¢åŠ èŠ‚ç‚¹èµ„æº

### å¦‚ä½•è°ƒæ•´ Memcached å†…å­˜é™åˆ¶

Loki Helm Chart æ”¯æŒé€šè¿‡ `allocatedMemory` å‚æ•°é…ç½® Memcached çš„å†…å­˜é™åˆ¶ï¼š

```yaml
chunksCache:
  enabled: true
  # è°ƒæ•´ Memcached åˆ†é…çš„å†…å­˜ï¼ˆMBï¼‰
  # è¿™ä¸ªå€¼åº”è¯¥å°äºæˆ–ç­‰äº Kubernetes limits.memory
  allocatedMemory: 1024 # 1GBï¼Œä»é»˜è®¤çš„ 8192MB å‡å°‘
  maxItemMemory: 5 # MBï¼Œæœ€å¤§ item å¤§å°
  connectionLimit: 16384
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      memory: 2Gi # åº”è¯¥å¤§äº allocatedMemory
```

**é‡è¦**ï¼š

- `allocatedMemory` æ˜¯ Memcached å®é™…å¯ç”¨çš„å†…å­˜ï¼ˆMBï¼‰
- åº”è¯¥å°äºæˆ–ç­‰äº Kubernetes `limits.memory`
- å¦‚æœ `allocatedMemory` å¤§äº `limits.memory`ï¼ŒMemcached å¯èƒ½è¢« OOMKilled
- å»ºè®®ï¼š`allocatedMemory` â‰¤ `limits.memory` - 200MBï¼ˆç•™å‡ºç³»ç»Ÿå¼€é”€ï¼‰

### æ€»ç»“

- **chunksCache çš„ä½œç”¨**ï¼šç¼“å­˜æ—¥å¿—æ•°æ®å—ï¼ŒåŠ é€ŸæŸ¥è¯¢ï¼Œå‡å°‘ S3 è®¿é—®
- **é»˜è®¤å¤§å†…å­˜çš„åŸå› **ï¼šMemcached é»˜è®¤é…ç½®éœ€è¦ 8GB å†…å­˜ï¼ŒåŠ ä¸Šç³»ç»Ÿå¼€é”€çº¦ 9.6GB
- **å‡å°‘èµ„æºçš„å½±å“**ï¼šç¼“å­˜å®¹é‡å˜å°ï¼Œå¯èƒ½å½±å“å¤§è§„æ¨¡/é«˜å¹¶å‘åœºæ™¯çš„æ€§èƒ½
- **å»ºè®®**ï¼šæ ¹æ®å®é™…ä½¿ç”¨åœºæ™¯è°ƒæ•´ï¼Œæµ‹è¯•ç¯å¢ƒå¯ä»¥å°ï¼Œç”Ÿäº§ç¯å¢ƒå»ºè®®ä¿æŒè¾ƒå¤§

---

## ğŸ”§ é€šç”¨æ’æŸ¥æ­¥éª¤

### 1. æ£€æŸ¥ ArgoCD Application çŠ¶æ€

```bash
# æŸ¥çœ‹æ‰€æœ‰åº”ç”¨çŠ¶æ€
kubectl get application -n argocd

# æŸ¥çœ‹è¯¦ç»†çŠ¶æ€å’Œé”™è¯¯ä¿¡æ¯
kubectl describe application <app-name> -n argocd

# æŸ¥çœ‹åº”ç”¨äº‹ä»¶
kubectl get events -n argocd --field-selector involvedObject.name=<app-name>
```

### 2. æ£€æŸ¥ ArgoCD æ—¥å¿—

```bash
# æŸ¥çœ‹ ArgoCD Repo Server æ—¥å¿—ï¼ˆå¤„ç† Git å’Œ Helm ä»“åº“ï¼‰
kubectl logs -n argocd deployment/argocd-repo-server --tail=100

# æŸ¥çœ‹ ArgoCD Application Controller æ—¥å¿—
kubectl logs -n argocd deployment/argocd-application-controller --tail=100
```

### 3. æ£€æŸ¥ Pod çŠ¶æ€

```bash
# æ£€æŸ¥ç‰¹å®šå‘½åç©ºé—´çš„ Pod
kubectl get pods -n <namespace>

# æŸ¥çœ‹ Pod è¯¦ç»†ä¿¡æ¯å’Œäº‹ä»¶
kubectl describe pod <pod-name> -n <namespace>

# æŸ¥çœ‹ Pod æ—¥å¿—
kubectl logs <pod-name> -n <namespace>
```

### 4. éªŒè¯ Git ä»“åº“è¿æ¥

```bash
# åœ¨ ArgoCD UI ä¸­æ£€æŸ¥
# Settings â†’ Repositories â†’ æŸ¥çœ‹ä»“åº“è¿æ¥çŠ¶æ€

# æˆ–ä½¿ç”¨ ArgoCD CLI
argocd repo list
```

### 5. æ‰‹åŠ¨è§¦å‘åŒæ­¥

```bash
# ä½¿ç”¨ kubectl
kubectl patch application <app-name> -n argocd \
  --type merge -p '{"operation":{"initiatedBy":{"username":"admin"},"sync":{"revision":"main"}}}'

# æˆ–ä½¿ç”¨ ArgoCD CLI
argocd app sync <app-name>
```

---

## ğŸ“ ä¿®å¤åçš„é…ç½®æ£€æŸ¥æ¸…å•

- [ ] Loki é…ç½®åŒ…å« `deploymentMode: SingleBinary` å’Œ `singleBinary.enabled: true`
- [ ] Loki é…ç½®ç¦ç”¨äº†å…¶ä»–æ¨¡å¼ï¼ˆsimpleScalable, read, write, backendï¼‰
- [ ] nginx-app.yaml ä½¿ç”¨ `sources`ï¼ˆå¤æ•°ï¼‰å¹¶åŒ…å« Git ä»“åº“
- [ ] Grafana é…ç½®**å®Œå…¨ç§»é™¤äº† `admin` éƒ¨åˆ†**ï¼ˆä¸åªæ˜¯æ³¨é‡Šï¼‰
- [ ] Grafana é…ç½®åªä¿ç•™ `secret` éƒ¨åˆ†
- [ ] Grafana æ•°æ®æºé…ç½®ä¸­ï¼Œåªæœ‰ä¸€ä¸ªæ•°æ®æºè®¾ç½®äº† `isDefault: true`
- [ ] å…¶ä»–æ•°æ®æºï¼ˆå¦‚ Lokiï¼‰çš„ `isDefault` è®¾ç½®ä¸º `false`
- [ ] ArgoCD Server Service å·²é…ç½®ä¸º LoadBalancerï¼ˆå¦‚æœéœ€è¦å¤–éƒ¨è®¿é—®ï¼‰
- [ ] æ‰€æœ‰å­˜å‚¨ç±»é…ç½®æ­£ç¡®ï¼ˆæ ¹æ®å®é™…ç¯å¢ƒä¿®æ”¹ï¼‰
- [ ] Git ä»“åº“ URL æ­£ç¡®
- [ ] æ‰€æœ‰ values æ–‡ä»¶å·²æäº¤åˆ° Git ä»“åº“

---

## ğŸš€ å¿«é€Ÿä¿®å¤å‘½ä»¤

å¦‚æœé‡åˆ°ç›¸åŒé—®é¢˜ï¼Œå¯ä»¥å¿«é€Ÿåº”ç”¨ä¿®å¤ï¼š

```bash
# 1. æ‹‰å–æœ€æ–°ä»£ç 
git pull origin main

# 2. åº”ç”¨ä¿®å¤åçš„é…ç½®
kubectl apply -f monitoring/argocd/loki.yaml
kubectl apply -f monitoring/argocd/prometheus.yaml
kubectl apply -f test-app/argocd/nginx-app.yaml

# 3. é…ç½® ArgoCD LoadBalancerï¼ˆå¦‚æœéœ€è¦å¤–éƒ¨è®¿é—®ï¼‰
kubectl apply -f argocd/argocd-server-service.yaml

# 4. ç­‰å¾…åŒæ­¥å®Œæˆ
kubectl get application -n argocd -w

# 5. æ£€æŸ¥ Pod çŠ¶æ€
kubectl get pods -n monitoring
kubectl get pods -n test-app

# 6. æ£€æŸ¥ Service çŠ¶æ€
kubectl get svc -n argocd argocd-server
kubectl get svc -n monitoring prometheus-grafana
```

---

## ğŸ“š å‚è€ƒèµ„æº

- [Loki Helm Chart æ–‡æ¡£](https://github.com/grafana/helm-charts/tree/main/charts/loki)
- [ArgoCD Multi-Source Applications](https://argo-cd.readthedocs.io/en/stable/user-guide/multiple_sources/)
- [Grafana Helm Chart æ–‡æ¡£](https://github.com/grafana/helm-charts/tree/main/charts/grafana)
- [ArgoCD æ•…éšœæ’æŸ¥](https://argo-cd.readthedocs.io/en/stable/operator-manual/troubleshooting/)
